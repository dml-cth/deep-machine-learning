{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fa9f96d4ca0144b2db877078cf7b2f8",
     "grade": false,
     "grade_id": "cell-5690119ead85e67e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Checklist for submission\n",
    "\n",
    "It is extremely important to make sure that:\n",
    "\n",
    "1. Everything runs as expected (no bugs when running cells);\n",
    "2. The output from each cell corresponds to its code (don't change any cell's contents without rerunning it afterwards);\n",
    "3. All outputs are present (don't delete any of the outputs);\n",
    "4. Fill in all the places that say `# YOUR CODE HERE`, or \"**Your answer:** (fill in here)\".\n",
    "5. Never copy/paste any notebook cells. Inserting new cells is allowed, but it should not be necessary.\n",
    "6. The notebook contains some hidden metadata which is important during our grading process. **Make sure not to corrupt any of this metadata!** The metadata may for example be corrupted if you copy/paste any notebook cells, or if you perform an unsuccessful git merge / git pull. It may also be pruned completely if using Google Colab, so watch out for this. Searching for \"nbgrader\" when opening the notebook in a text editor should take you to the important metadata entries.\n",
    "7. Although we will try our very best to avoid this, it may happen that bugs are found after an assignment is released, and that we will push an updated version of the assignment to GitHub. If this happens, it is important that you update to the new version, while making sure the notebook metadata is properly updated as well. The safest way to make sure nothing gets messed up is to start from scratch on a clean updated version of the notebook, copy/pasting your code from the cells of the previous version into the cells of the new version.\n",
    "8. If you need to have multiple parallel versions of this notebook, make sure not to move them to another directory.\n",
    "9. Although not forced to work exclusively in the course `conda` environment, you need to make sure that the notebook will run in that environment, i.e. that you have not added any additional dependencies.\n",
    "\n",
    "**FOR HA1, HA2, HA3 ONLY:** Failing to meet any of these requirements might lead to either a subtraction of POEs (at best) or a request for resubmission (at worst).\n",
    "\n",
    "We advise you to perform the following steps before submission to ensure that requirements 1, 2, and 3 are always met: **Restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). This might require a bit of time, so plan ahead for this (and possibly use Google Cloud's GPU in HA1 and HA2 for this step). Finally press the \"Save and Checkout\" button before handing in, to make sure that all your changes are saved to this .ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6bb874a16c1ff767ac0f37ce0491265",
     "grade": false,
     "grade_id": "cell-774c93bf6433de68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in name of notebook file\n",
    "This might seem silly, but the version check below needs to know the filename of the current notebook, which is not trivial to find out programmatically.\n",
    "\n",
    "You might want to have several parallel versions of the notebook, and it is fine to rename the notebook as long as it stays in the same directory. **However**, if you do rename it, you also need to update its own filename below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_fname = \"XXX.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "879883c2ea755808ffd00aeee5c77a00",
     "grade": false,
     "grade_id": "cell-5676bcf768a7f9be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in group number and member names (use NAME2 and GROUP only for HA1, HA2 and HA3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME1 = \"\" \n",
    "NAME2 = \"\"\n",
    "GROUP = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42f960a95815e1aa3ce8132fcec59cd9",
     "grade": false,
     "grade_id": "cell-a15fe781533d9590",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a5517d7993b4b35049f0013dd6a3f55",
     "grade": false,
     "grade_id": "cell-2b9c2390ee464c39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from platform import python_version_tuple\n",
    "assert python_version_tuple()[:2] == ('3','7'), \"You are not running Python 3.7. Make sure to run Python through the course Conda environment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15ec4309f1e85f6e17bda73b9b6f48a2",
     "grade": false,
     "grade_id": "cell-4869b45600ce82f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check that notebook server has access to all required resources, and that notebook has not moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2d199303c73ec86d25177caf39e385f",
     "grade": false,
     "grade_id": "cell-122ac3d9100b8afb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "nb_dirname = os.path.abspath('')\n",
    "assignment_name = os.path.basename(nb_dirname)\n",
    "assert assignment_name in ['IHA1', 'IHA2', 'HA1', 'HA2', 'HA3'], \\\n",
    "    '[ERROR] The notebook appears to have been moved from its original directory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f09f40b5350db83232189137c550f0a1",
     "grade": false,
     "grade_id": "cell-2455deee513cd39c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify correct nb_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a78c7227b049bb147e6c363affb6dae8",
     "grade": false,
     "grade_id": "cell-0472e2fd710f1d72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "try:\n",
    "    display(HTML(r'<script>if(\"{nb_fname}\" != IPython.notebook.notebook_name) {{ alert(\"You have filled in nb_fname = \\\"{nb_fname}\\\", but this does not seem to match the notebook filename \\\"\" + IPython.notebook.notebook_name + \"\\\".\"); }}</script>'.format(nb_fname=nb_fname)))\n",
    "except NameError:\n",
    "    assert False, 'Make sure to fill in the nb_fname variable above!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98d88d8e8da19693053764f29dcc591d",
     "grade": false,
     "grade_id": "cell-ceacb1adcae4783d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify that your notebook is up-to-date and not corrupted in any way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb700528d4644601c1a8c91ef1d84635",
     "grade": false,
     "grade_id": "cell-f5a59288e11b4aec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from ha_utils import check_notebook_uptodate_and_not_corrupted\n",
    "check_notebook_uptodate_and_not_corrupted(nb_dirname, nb_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be161193815d6c8b524adf7b70ce7543",
     "grade": false,
     "grade_id": "cell-44e7522eff275a3b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "![test](data/translate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b79723d87eadd780ec89fa4452ccc87",
     "grade": false,
     "grade_id": "cell-3c556a39514d3d6c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Task 2 - Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "894ae9ce85da0384a36fb57960330fa1",
     "grade": false,
     "grade_id": "cell-1531dbb9354dac88",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In this task you will build a system that uses recurrent neural networks to translate sentences from English to Spanish! \n",
    "\n",
    "Before we start coding anything, it's important to clear up some details about neural machine translation and make sure everyone is on the same page regarding conventions, etc. Please read the next section carefully and make sure you understand the specifics of the solution we will develop in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "409886b827fd4c4421e5946574a6958f",
     "grade": false,
     "grade_id": "cell-a4bff0f2271fff88",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.0 Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0af648058036adc9d87713a4b89876c0",
     "grade": false,
     "grade_id": "cell-1d1463757139054a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "For this translation task, we'll use an encoder-decoder architecture. The input sentence (in English) will be first processed by the encoder, which will output its hidden state at the last time-step. After that, this hidden state will be fed as the initial state for the decoder, which will generate the output sentence (in Spanish). The following diagram illustrates the process:\n",
    "\n",
    "![nmt_teacher_forcing](data/nmt_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d2e5b2678c07d902f4027f108cf5fc5",
     "grade": false,
     "grade_id": "cell-c9f6b1971b7c0493",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We feed one word of the input sentence at a time (first `i`, then `am`, then `happy`) to the encoder, which processes it and computes its next hidden state. Once the last word of the input sentence is processed by the encoder, we feed its current hidden state to the decoder.\n",
    "\n",
    "The decoder then uses this hidden state as initial state, together with a special word token that we feed as input, which signifies the \"start of sentence\", `<SOS>`. This tells the decoder it should start generating the output sentence.\n",
    "\n",
    "The decoder then produces a probability distribution over possible spanish words for the first position. We sample from this distribution to obtain one word, which in this case the sample was `estoy`. This output is now the first word of the generated translation. In order to generate the next word, we feed the sample `estoy` as input to the decoder in the next time-step, which then produces a new probability distribution over possible spanish words again, from which we sampled `feliz` in the example. This process continues until it produces the special word token `<EOS>`, which signifies the end of the sentence (or until a maximum number of words have been generated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ad4295974edfe6dd20f1ec1d1dbc5e0",
     "grade": false,
     "grade_id": "cell-5dfda0f0cbf95851",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Because of this connection from output to input, this type of architecture can be challenging to train. One commonly used technique to circumvent this problem is called *teacher forcing*. In it, instead of feeding the output at time-step $t$ as input to time-step $t+1$, we use as input the *ground-truth label* for time-step $t+1$. The following diagram illustrates it:\n",
    "\n",
    "![nmt_teacher_forcing](data/nmt_teacher_forcing.png)\n",
    "\n",
    "Note how in this example the model incorrectly predicted `estas` as the first word in the translation. However, instead of feeding it as input to the next time-step, and making it more likely to also misclassify the next word, we feed the *ground-truth label* as input, `estoy`, here shown in green. We continue this process until we fed all of the ground-truth words to the decoder (regardless of whether the model predicts `<EOS>` or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9949acfd493ff4800c33f8e66923f05",
     "grade": false,
     "grade_id": "cell-438e7e0fcd4c4091",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.1 Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce9b564cefed1357a0db712c6d9a68be",
     "grade": false,
     "grade_id": "cell-4af1474eab90f437",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now that it's clear which kind of system we will develop, we can start the task by loading the data we will use.\n",
    "\n",
    "The data for this task is present in the file `data/eng-spa.txt` (obtained from [here](https://www.manythings.org/anki/)), a text file comprised of thousands of sentences in English and their counterpart Spanish translations. Before continuing, take a look at the file now to see its internal structure.\n",
    "\n",
    "We'll load the sentences in this file using the provided helper function `get_data_from_file`. This function loads a subset of the entire dataset (no long sentences, no sentences with rare words, etc) for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6eb5a27fde7a820765d9daf3f55fae3e",
     "grade": false,
     "grade_id": "cell-f846c494c826a310",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from utils.load_sentences import get_data_from_file, SOS_word_idx, EOS_word_idx, PAD_word_idx\n",
    "pairs, input_lang, output_lang = get_data_from_file('data/eng-spa.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20a426fcdf0ba683bf63369d52e7a418",
     "grade": false,
     "grade_id": "cell-c2fb9d37d6959f02",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The first loaded object, `pairs`, is a list of all the loaded pairs of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f9cb9d771b742b30ef13947a8e822b5",
     "grade": false,
     "grade_id": "cell-1a58d8ec5834578f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cf7e1869f95a864bf11812110126b53",
     "grade": false,
     "grade_id": "cell-06c2f57520370ab9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8cbaa67546e939414b5752e0a95c0078",
     "grade": false,
     "grade_id": "cell-c9cb3ec23bbe5c66",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "`input_lang` and `output_lang` are helper objects that contain important information about the languages loaded, and also dictionaries to map between words and indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df9f9b4ebfc127fa4f76a0f17acde741",
     "grade": false,
     "grade_id": "cell-06fb5b83929ba750",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "input_lang.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fdab8e48137b1678d343fc9c8b529a8f",
     "grade": false,
     "grade_id": "cell-710f5b42e5f82f03",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "input_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64578e01bbd6c020a26648947532977c",
     "grade": false,
     "grade_id": "cell-062c251db2d72892",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "input_lang.word2index['great']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3a0c2e700574be1e021bf516da051fc",
     "grade": false,
     "grade_id": "cell-2bcfc45667215f16",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "input_lang.index2word[702]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9e3f0543a9c1e7110ebac1317a86807",
     "grade": false,
     "grade_id": "cell-7130ad219189a0d0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Finally, we also loaded some variables that define indexes for the three special word tokens we'll add to the sentences:\n",
    "- `PAD`: Padding;\n",
    "- `SOS`: Start of sentence;\n",
    "- `EOS`: End of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d458678d06ea2a89c33b3aee6e3248d4",
     "grade": false,
     "grade_id": "cell-08a85ba00b002bf5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "PAD_word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16d10a92964cc2c62094d56eec31780a",
     "grade": false,
     "grade_id": "cell-9d07489bcff74552",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "SOS_word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79ce628f7163f1464f825ce1d81ce472",
     "grade": false,
     "grade_id": "cell-332b49fe883bd8e0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "EOS_word_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "653133d19854fcb6ecaf32dac1a69ff6",
     "grade": false,
     "grade_id": "cell-51d03d46a4730112",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.2 Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2193bb0434c4d8f7009e6079b045023a",
     "grade": false,
     "grade_id": "cell-39d421f43d0c02c4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We'll pre-process the data in a similar way to task 1. The following function transforms a sentence into a tensor by changing each word into its corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fe3f7938e5256adb6c8e2b90d944781",
     "grade": false,
     "grade_id": "cell-2a548cd9fdb03d8f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sentence2tensor(lang, sentence):\n",
    "    return torch.tensor([lang.word2index[word] for word in sentence.split(' ')], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d4fdd9429729e29d70c46fcccd2d89f",
     "grade": false,
     "grade_id": "cell-9a0498b4a61bc45e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sentence2tensor(input_lang, 'this is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "243a3d2e48c38b80f1efba354a165cef",
     "grade": false,
     "grade_id": "cell-17aa1999bb0519d6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "input_lang.index2word[512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8110be2912e9ba135e686572c43a6c2",
     "grade": false,
     "grade_id": "cell-746bafddcbfd9aba",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We also want to pad the sentences, so that all have equal length - allowing us to batch several sentences together and speed up the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "446909d1539c0215ff184090fca8d428",
     "grade": false,
     "grade_id": "cell-4bb35eab7404cf63",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "def pad_all_sequences(sequences, padding_value, pad_on_the_left=False):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        sequences        - List of 1-D LongTensor elements of variable length, each tensor representing one sequence.\n",
    "        padding_value    - Integer value, with which to pad the sequences.\n",
    "        pad_on_the_left  - bool. If True, pad on the left-hand-side instead of right-hand-side.\n",
    "    Returns:\n",
    "        Tensor of shape (nbr_sequences, max_len), where max_len is the length of the padded sequences. nbr_sequences is the number of sequences provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    if pad_on_the_left:\n",
    "        # Flip each sequence along its one and only dimension (dim=0)\n",
    "        sequences = [torch.flip(seq_tensor, [0]) for seq_tensor in sequences]\n",
    "        \n",
    "    tensor_of_padded_seqs = nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=padding_value)\n",
    "    \n",
    "    if pad_on_the_left:\n",
    "        # Flip sequences back, along the sequential dimension (now dim=1)\n",
    "        tensor_of_padded_seqs = torch.flip(tensor_of_padded_seqs, [1])\n",
    "        \n",
    "\n",
    "    return tensor_of_padded_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "054b729d4c2a7240df5e841bf7409541",
     "grade": false,
     "grade_id": "cell-648a50a17d0269e2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Note that the function `pad_all_sequences` takes an argument named `pad_on_the_left`, which if set to `True`, pads the sequences on the left, instead of the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7f9d8c86eb39a3b8ff3bc5b8caa08a1",
     "grade": false,
     "grade_id": "cell-d351e9c8c79ee5df",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_sentences = ['this is a great test', 'this too', 'yet another test']\n",
    "test_sequences = [sentence2tensor(input_lang, s) for s in test_sentences]\n",
    "print('Before padding:')\n",
    "for s in test_sequences:\n",
    "    print(s)\n",
    "    \n",
    "print('\\nAfter padding:')\n",
    "for s in pad_all_sequences(test_sequences, PAD_word_idx):\n",
    "    print(s)\n",
    "    \n",
    "print('\\nAfter padding (on the left):')\n",
    "for s in pad_all_sequences(test_sequences, PAD_word_idx, pad_on_the_left=True):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f455354ccfc0a49bb7d94e8c32b3819",
     "grade": false,
     "grade_id": "cell-eeac94c3ecfcc5e8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Question:**\n",
    "Think about the encoder-decoder architecture commonly used for translation. In this context, why would we want to pad some sentences on the left and some on the right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a8e65f983cecadcb366b2b553439d00",
     "grade": true,
     "grade_id": "cell-e1459474d3e61f00",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f09b79e8196a1032081ee6840e604cb8",
     "grade": false,
     "grade_id": "cell-4473215afd0827d8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we use `sentence2tensor` and `pad_all_sequences` to transform the `pairs` object into input and output tensors of word indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f412e8aca812caacb0af72a18f40332",
     "grade": false,
     "grade_id": "cell-535e46dd571b9284",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Separate input and target sentences from pairs\n",
    "input_sentences, target_sentences = zip(*pairs)\n",
    "\n",
    "# Create tensors, appending the <SOS> and <EOS> words to the target sequences.\n",
    "input_seq_tensors = [torch.tensor([input_lang.word2index[word] for word in sentence.split(' ')], \n",
    "                                  dtype=torch.long) \n",
    "                     for sentence in input_sentences]\n",
    "target_seq_tensors = [torch.tensor([SOS_word_idx] + [output_lang.word2index[word] for word in sentence.split(' ')] + [EOS_word_idx], \n",
    "                                   dtype=torch.long) \n",
    "                      for sentence in target_sentences]\n",
    "\n",
    "# Pad all sequences to equal length\n",
    "input_seq_tensors = pad_all_sequences(input_seq_tensors, PAD_word_idx, pad_on_the_left=True)\n",
    "target_seq_tensors = pad_all_sequences(target_seq_tensors, PAD_word_idx, pad_on_the_left=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e4490be9d0d4ddb0e708c41b0039ee6",
     "grade": false,
     "grade_id": "cell-c6938740fc6cf9db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Note that we're adding the `SOS` token to the start of all the target sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c577ca87b1c323f98429d21eb03c1da8",
     "grade": false,
     "grade_id": "cell-9b67db79adeb75d3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "input_seq_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d632a2b84fae850bc98a39c96b1d9cc8",
     "grade": false,
     "grade_id": "cell-a68000a248c4d4f7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "target_seq_tensors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2668f104b3f8683cfeb922c90d1b1168",
     "grade": false,
     "grade_id": "cell-5fcb1a036ccf47fc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Using these tensors, we create a dataset and split it into train/val/test as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b77be122f6731d3c14d60683d8b0039c",
     "grade": false,
     "grade_id": "cell-782a05ee9fec69cb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "dataset = TensorDataset(input_seq_tensors, target_seq_tensors)\n",
    "\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "n_samples = len(dataset)\n",
    "n_val_samples = int(n_samples*val_ratio)\n",
    "n_test_samples = int(n_samples*test_ratio)\n",
    "n_train_samples = n_samples-n_val_samples-n_test_samples\n",
    "\n",
    "# Fix RNG seed\n",
    "old_state = torch.get_rng_state()\n",
    "torch.manual_seed(0)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [n_train_samples, n_val_samples, n_test_samples])\n",
    "torch.set_rng_state(old_state)\n",
    "print(old_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0df47891586ccab73dc10b95b21a33e2",
     "grade": false,
     "grade_id": "cell-5554838bcc3faced",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da50cc62be492f949b10d8587783b572",
     "grade": false,
     "grade_id": "cell-d1ffc03b003a1689",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.3 Defining the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de0abd99fef7dbbd1ecbd385d0a82deb",
     "grade": false,
     "grade_id": "cell-ff83666a724761f0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As stated before, we'll tackle this translation task with an encoder-decoder architecture. In this section, you will define the `Encoder` and `Decoder` classes, and also the `Translator` class, that combines the other two to translate a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2b67acab17f1e6106ad90daee6b484e",
     "grade": false,
     "grade_id": "cell-957262dc380f1c7c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02de08dc94e23ef43bbba5b7f5f20d1d",
     "grade": false,
     "grade_id": "cell-db83c03e65ea6f71",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### 2.3.1 Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96da9b98f01ca49f8deafe1bf0b7496d",
     "grade": false,
     "grade_id": "cell-80c4afb2321f429a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We will start with the `Encoder` class. Its `forward` method should take as input a batch of tensors of word indexes and perform the following steps:\n",
    "\n",
    "- Embed the input tensors using the [`Embedding`](https://pytorch.org/docs/stable/nn.html#embedding) PyTorch layer.\n",
    "- Feed the embedded tensors into a multi-layer [`GRU` module](https://pytorch.org/docs/stable/nn.html#gru).\n",
    "- Output the hidden states of all GRU layers at the last time-step of the sequence (i.e. for the last word).\n",
    "\n",
    "*Tips*:\n",
    "- Read the entire documentation for both the [`Embedding`](https://pytorch.org/docs/stable/nn.html#embedding) and the [`GRU`](https://pytorch.org/docs/stable/nn.html#gru) layers.\n",
    "- Don't be confused: in task 1, when predicting nationalities, we created a module with a `forward` method that should be called for each element in the input sequence; *this is not the case for task 2*. Here we want the `forward` method of `Encoder` to run in the entire sequence with one call.\n",
    "- The `forward` method of the `GRU` module can be run in the entire sequence with one call (but you already know this, since you followed the first tip we mentioned üòÅ). Also, the `GRU` constructor has an argument named `batch_first`...\n",
    "- Initialize the hidden state of the `GRU`s to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53a164d71aafd1abc3004c7b86d6adfa",
     "grade": true,
     "grade_id": "cell-34c7a1017392b6bc",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_size     - Number of unique word indexes in the input language (i.e. input_lang.n_words)\n",
    "            embedding_size - Dimensionality of the space the input tensors will be embedded before going into the\n",
    "                             recurrent unit.\n",
    "            hidden_size    - Dimensionality of the hidden vector in the GRUs.\n",
    "            num_layers     - Number of layers in the multi-layer GRU unit.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Runs the RNN encoder.\n",
    "        Inputs:\n",
    "            x              - Input sequence of word indexes. \n",
    "                             LongTensor, shape (batch_size, seq_len).\n",
    "        Returns:\n",
    "            new_h          - Hidden state at current time-step for all layers at the final sequence position.\n",
    "                             FloatTensor, shape (num_layers, batch_size, hidden_size).\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return new_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7bf289fc9b82a4d607e234c9b348e98a",
     "grade": false,
     "grade_id": "cell-ae42248d6749eff6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Run the following cell to test whether the shape of your outputs are correct. Note that this *only* checks the shapes, not the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "823396af31ee17eca7702fab6924b115",
     "grade": false,
     "grade_id": "cell-08639295de373894",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_batch_size = 32\n",
    "test_num_layers = 3\n",
    "\n",
    "# Create encoder\n",
    "test_encoder = Encoder(input_size=input_lang.n_words, embedding_size=30, hidden_size=50, num_layers=test_num_layers)\n",
    "test_encoder.to(device)\n",
    "\n",
    "# Create dummy input\n",
    "test_x = torch.zeros(test_batch_size, 10, dtype=torch.long, device=device)\n",
    "\n",
    "# Forward-prop through the encoder\n",
    "test_new_h = test_encoder(test_x)\n",
    "\n",
    "# Test the shape of the output\n",
    "err_str = '`test_new_h` has incorrect dimensions. Dimension is {}, but should\\'ve been {}'\n",
    "correct_dimension = (test_num_layers, test_batch_size, test_encoder.hidden_size)\n",
    "assert test_new_h.shape == correct_dimension, err_str.format(tuple(test_new_h.shape), correct_dimension)\n",
    "print('Test passed. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84926f90db7db663adeb3a19680d35e1",
     "grade": false,
     "grade_id": "cell-59960950d507a5b7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### 2.3.2 Decoder\n",
    "\n",
    "Now we move to the `Decoder` class. \n",
    "\n",
    "Its `forward` method takes the hidden state output from the `Encoder` as input, and predicts the word indexes corresponding to the translated sentence. Furthermore, since we will train our translator using teacher-forcing, this method also takes as input a sequence of word indexes corresponding to the ground-truth output sentence. \n",
    "\n",
    "The following steps need to be performed by the `forward` method:\n",
    "\n",
    "- Embed the ground-truth output sequence using an `Embedding` layer.\n",
    "- Feed the embedded sequence to a multi-layer `GRU` module.\n",
    "- Pass the computed hidden states of the `GRU` module at each time-step through a linear layer\n",
    "- Pass the outputs of the linear layer at each time-step through a `LogSoftmax` layer.\n",
    "\n",
    "The first output of this method is then a sequence numbers representing the probability mass functions over the possible output indexes for each time-step. The second output should be the hidden-states for all layers of the `GRU`s , for the last time-step of the sequence (i.e. for the last word index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06b2e266d0915396d0d719ec076f18fd",
     "grade": true,
     "grade_id": "cell-8602e5addd641b49",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, num_layers):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embedding_size - Dimensionality of the space the input tensors will be embedded before going into the\n",
    "                             recurrent unit.\n",
    "            hidden_size    - Dimensionality of the hidden vector in the GRUs.\n",
    "            output_size    - Number of unique word indexes in the output language (i.e. output_lang.n_words)\n",
    "            num_layers     - Number of layers in the multi-layer GRU unit.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x             - Sequence of word indexes corresponding to ground-truth output sentence. \n",
    "                            LongTensor, shape (batch_size, seq_len).\n",
    "            h             - Previous state (output from Encoder object). \n",
    "                            FloatTensor, shape (num_layers, batch_size, hidden_size).\n",
    "\n",
    "        Outputs:\n",
    "            out           - Output sequence of predicted distributions of target words.\n",
    "                            FloatTensor, shape (batch_size, seq_len, output_size).\n",
    "            new_h         - Hidden state at current time-step.\n",
    "                            FloatTensor, shape (num_layers, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        return out, new_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08647fa8ab35680eac0d5e10fc1efac0",
     "grade": false,
     "grade_id": "cell-4eed24b8e8f0279c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Run the following cell to both test the shapes of your outputs and also that the predicted distributions sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24c084d0ca2200027df6b4ef16a7798d",
     "grade": false,
     "grade_id": "cell-768c00536daaf170",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "test_batch_size = 6\n",
    "test_seq_len = 7\n",
    "\n",
    "# Create encoder\n",
    "test_decoder = Decoder(embedding_size=300, \n",
    "                       hidden_size=30, \n",
    "                       output_size=output_lang.n_words, \n",
    "                       num_layers=3)\n",
    "test_decoder.to(device)\n",
    "\n",
    "# Create dummy input and hidden state\n",
    "test_x = torch.ones(test_batch_size, test_seq_len, dtype=torch.long, device=device)\n",
    "test_h = torch.ones(test_decoder.num_layers, test_batch_size, test_decoder.hidden_size, device=device)\n",
    "\n",
    "# Forward-prop through the decoder\n",
    "out, test_new_h = test_decoder(test_x, test_h)\n",
    "\n",
    "# Does the new hidden state have the correct shape?\n",
    "err_msg = '{} has incorrect dimensions. Dimension is {}, but should\\'ve been {}'\n",
    "correct_shape = (test_decoder.num_layers, test_batch_size, test_decoder.hidden_size)\n",
    "assert test_new_h.shape == correct_shape, err_msg.format('`test_new_h`', tuple(test_new_h.shape), correct_shape)\n",
    "\n",
    "# Does the output have the correct shape?\n",
    "correct_shape = (test_batch_size, test_seq_len, output_lang.n_words)\n",
    "assert out.shape == correct_shape, err_msg.format('`out`', tuple(out.shape), correct_shape)\n",
    "\n",
    "# Do the predictions sum to 1?\n",
    "test_pred_sums = torch.sum(out.exp(), dim=2).detach().cpu().numpy()\n",
    "err_msg = 'Predictions do not sum to 1:\\n {}'.format(test_pred_sums)\n",
    "assert np.allclose(test_pred_sums, np.ones_like(test_pred_sums)),  err_msg\n",
    "\n",
    "print('Test passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b7c040cc971a1f5468cf40d76fac9bf",
     "grade": false,
     "grade_id": "cell-aade27372bb6c7ac",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### 2.3.4 Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "701cbd99222d97de73dc372d76583684",
     "grade": false,
     "grade_id": "cell-aeb3fcebf5a4cb1b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now that we defined the `Encoder` and the `Decoder` classes, we're ready to define the full model that connects these two together, the `Translator` class.\n",
    "\n",
    "This class has two methods, `forward` and `forward_no_teacher`. \n",
    "\n",
    "The first one is the one you will implement, which takes as input a sequence of word indexes corresponding to the input sentence. This sequence is fed to the `Translator`'s encoder, which outputs its last hidden state. Finally, this hidden state is then fed to the `Translator`'s decoder, together with the ground-truth word indexes corresponding to the ground-truth output sentence, resulting in the predicted word distributions for each position. This is the method we will use to compute predictions during training.\n",
    "\n",
    "The second method, `forward_no_teacher`, is already implemented for you. This method performs the same steps as outlined above, but this time without using the ground-truth output sequence for teacher forcing. Instead, the output of the decoder at time-step $t$ is fed to itself at time-step $t+1$ (instead of the ground-truth word index for this time-step). We will use this method for evaluating the final quality of our translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "463efb56f55aba53792d83df1463fb5b",
     "grade": true,
     "grade_id": "cell-4c72b777d53ebc3c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Translator(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_size     - Number of unique word indexes in the input language (i.e. input_lang.n_words)\n",
    "            embedding_size - Dimensionality of the space the input tensors will be embedded before going into the\n",
    "                             recurrent unit.\n",
    "            hidden_size    - Dimensionality of the hidden vector in the GRUs.\n",
    "            output_size    - Number of unique word indexes in the output language (i.e. output_lang.n_words)\n",
    "            num_layers     - Number of layers in the multi-layer GRU unit.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "                \n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x                - Sequence of word indexes corresponding to input sentence. \n",
    "                               LongTensor, shape (batch_size, input_seq_len).\n",
    "            y                - Sequence of word indexes corresponding to ground-truth output sentence.\n",
    "                               LongTensor, shape (batch_size, output_seq_len).\n",
    "\n",
    "        Outputs:\n",
    "            decoder_outputs  - Output sequence of predicted distributions of target words.\n",
    "                               FloatTensor, shape (batch_size, output_seq_len, output_size).\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return decoder_outputs    \n",
    "    \n",
    "    def forward_no_teacher(self, x, max_len=10):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Encode input sentence. Shape: (num_layers, batch_size, hidden_size)\n",
    "            last_hidden_encoder = self.encoder(x)\n",
    "\n",
    "            # Initialize tensor for sentence that will be generated\n",
    "            generated_sentence = torch.zeros(1, 0, dtype=torch.long, device=device)\n",
    "\n",
    "            # Initialize input (SOS) and hidden state (output hidden state of encoder) for the decoder\n",
    "            x = torch.ones(1, 1, dtype=torch.long, device=device)*SOS_word_idx\n",
    "            h = last_hidden_encoder        \n",
    "            \n",
    "            for i in range(max_len):\n",
    "                \n",
    "                # Compute output and new hidden state\n",
    "                out, h = self.decoder(x, h)\n",
    "                \n",
    "                # Choose the most probable word in the predicted pmf\n",
    "                x = out.argmax(dim=2)\n",
    "                \n",
    "                # Add this word to the generated sentence\n",
    "                generated_sentence = torch.cat([generated_sentence, x], dim=1)\n",
    "                \n",
    "                # Stop if generated word is EOS\n",
    "                if x.item() == EOS_word_idx:\n",
    "                    break\n",
    "\n",
    "            return generated_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ebf354aa97984a408f108ba9d2bf1de",
     "grade": false,
     "grade_id": "cell-dc2c9633f7affdf4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Run the following cell to both test the shapes of your outputs and also that the predicted distributions sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bc86dc737005063ea8dd4f7f567a458",
     "grade": false,
     "grade_id": "cell-1725aad33842598e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_batch_size = 12\n",
    "test_input_len = 8\n",
    "test_ground_truth_output_len = 6\n",
    "\n",
    "# Create translator\n",
    "test_translator = Translator(input_size=10, \n",
    "                            embedding_size=100, \n",
    "                            hidden_size=30, \n",
    "                            output_size=12, \n",
    "                            num_layers=3)\n",
    "test_translator.to(device)\n",
    "\n",
    "# Create test input sentence and output sentence\n",
    "test_input_sentence = torch.ones(test_batch_size, test_input_len, dtype=torch.long, device=device)\n",
    "test_ground_truth_output = torch.ones(test_batch_size, test_ground_truth_output_len, dtype=torch.long, device=device)\n",
    "\n",
    "# Forward-prop through the translator\n",
    "out = test_translator(test_input_sentence, test_ground_truth_output)\n",
    "\n",
    "# Does the output have the correct shape?\n",
    "correct_shape = (test_batch_size, test_ground_truth_output_len, test_translator.output_size)\n",
    "assert out.shape == correct_shape, err_msg.format('`out`', tuple(out.shape), correct_shape)\n",
    "\n",
    "# Do the predictions sum to 1?\n",
    "test_pred_sums = torch.sum(out.exp(), dim=2).detach().cpu().numpy()\n",
    "err_msg = 'Predictions do not sum to 1:\\n {}'.format(test_pred_sums)\n",
    "assert np.allclose(test_pred_sums, np.ones_like(test_pred_sums)),  err_msg\n",
    "\n",
    "print('Test passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e35c4cb9a70ce3063c0bfc0a3c8595a7",
     "grade": false,
     "grade_id": "cell-a2648e2cc194c3e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Which steps should be taken in order to make use of word2vec embeddings in our models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2cd149cb7bd826a46c09a01e710462f3",
     "grade": true,
     "grade_id": "cell-bbc68ff3a2bcbcba",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ce719f29c702bf16f00e76792ca3cb6",
     "grade": false,
     "grade_id": "cell-a3b7038020450c0f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The `forward_no_teacher` method will be used later for evaluating the translator's performance. Make sure you completely understand how it works before proceeding. The following cell shows some example usage. Since we didn't train the translator yet, its translations are mostly gibberish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "015698a5498f340228d1f7a2b8df898b",
     "grade": false,
     "grade_id": "cell-71cc095b9e95577c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Create test sentence\n",
    "test_input_sentence = 'this is a test'\n",
    "print('Input sentence:\\t\\t', test_input_sentence)\n",
    "\n",
    "# Map to tensor of word indexes and add batch dimension\n",
    "test_tensor = sentence2tensor(input_lang, test_input_sentence).to(device)\n",
    "test_tensor = test_tensor[None, :]\n",
    "\n",
    "# Create translator\n",
    "test_translator = Translator(input_size=input_lang.n_words,\n",
    "                             embedding_size=100,\n",
    "                             hidden_size=128,\n",
    "                             output_size=output_lang.n_words,\n",
    "                             num_layers=3)\n",
    "test_translator.to(device)\n",
    "\n",
    "# Translate the input sentence (generate at most 5 words in the translation)\n",
    "test_tensor_translated = test_translator.forward_no_teacher(test_tensor, 5).cpu().numpy()\n",
    "\n",
    "# Remove batch dimension\n",
    "test_tensor_translated = test_tensor_translated[0]\n",
    "\n",
    "# Map from indexes to words\n",
    "word_list = [output_lang.index2word[idx] for idx in test_tensor_translated]\n",
    "\n",
    "# Join all words in a string and print it\n",
    "output_sentence = ' '.join(word_list)\n",
    "\n",
    "print('Predicted translation:\\t', output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b01baef0aa516b117cdcf0a96f74554",
     "grade": false,
     "grade_id": "cell-859e5248b5907028",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Question:**\n",
    "Why do we get different translations every time we re-run the above cell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40d9794ba3e1e93f1a2a9719842ad574",
     "grade": true,
     "grade_id": "cell-82f800442f41bd8d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f1d43eaeeae5e6e1cc2da35fbe5efd7",
     "grade": false,
     "grade_id": "cell-912915a0b61dd530",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Question:**\n",
    "What would happen if you tried translating the sentence \"this is extraordinary\"? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b90d005cbe103a1a449b92f05505505",
     "grade": true,
     "grade_id": "cell-378967bd34f9ff95",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "110f0a7f77ec676451faab472cf7ffc2",
     "grade": false,
     "grade_id": "cell-cf018cd730905506",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.4 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d400bb66e04d92e996af02ea0171007c",
     "grade": false,
     "grade_id": "cell-51f306b5b4d1f042",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we're ready to train the model! Since at every time-step we're generating a prediction over possible words and comparing it to a ground-truth word, this can be seen as a classification problem. Because of this we'll use the `NLLLoss`, and the total loss will be the average loss value across all time-steps of the ground-truth sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f8f21aa527306e1805a661b44f5ec89",
     "grade": false,
     "grade_id": "cell-1d438d17fa64ff60",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10319f653d5d40ddf6a7d79210535455",
     "grade": false,
     "grade_id": "cell-4d4f92c545e608eb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Just like in task 1, we also first define a function for training in a single batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "596a98d969de1837ca312de4317def16",
     "grade": false,
     "grade_id": "cell-2521ffc63214fd17",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def train_batch(translator, input_sequences, target_sequences, optimizer):\n",
    "    \"\"\"\n",
    "    Performs forward & backward passes for one batch, and takes a gradient step.\n",
    "\n",
    "    Inputs:\n",
    "        translator         - Translator object\n",
    "        input_sequences    - Batch of input sequences.\n",
    "                             LongTensor, shape (batch_size, seq_len). Holding the source language word indexes.\n",
    "        target_sequences   - Batch of target sequences.\n",
    "                             LongTensor, shape (batch_size, seq_len). Holding the true target language word indexes.\n",
    "        optimizer          - Pytorch optimizer object\n",
    "    Returns:\n",
    "        The average loss for the batch (float)\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict distribution for each word position in the translated sentence\n",
    "    out = translator(input_sequences, target_sequences)\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Calculate loss\n",
    "    out = out.permute(0, 2, 1)\n",
    "    loss = loss_fn(out[:, :, :-1], target_sequences[:, 1:])\n",
    "    \n",
    "    # Backward-prop and perform one optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "678fc76ec1d0e2c2d3e33f88b981bb81",
     "grade": false,
     "grade_id": "cell-5b74ad9ee3d2d964",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Question:**\n",
    "Note that before computing the loss for a batch, we permute the dimensions of the output prediction from the translator. Why do we need to do this? \n",
    "\n",
    "*Tip*: read the [NLLLoss documentation](https://pytorch.org/docs/stable/nn.html#nllloss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "502d48ad7e97c60edca41a0569cfe2f6",
     "grade": true,
     "grade_id": "cell-ac30162be0f793e2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "264e12818fee2a587950260e1276a9e0",
     "grade": false,
     "grade_id": "cell-0125e2d37c03c980",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We also define functions for computing our metrics of interest in the validation set and for plotting the metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad26c59c8de810943f4c1ad8697b623f",
     "grade": false,
     "grade_id": "cell-0ab220dc995eea05",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics_on_validation_set(translator, val_data_loader):\n",
    "\n",
    "    n_processed, loss = 0, 0\n",
    "    for x_val, y_val in val_data_loader:\n",
    "        batch_size = x_val.shape[0]\n",
    "\n",
    "        # Put data in the correct device\n",
    "        x_val = x_val.to(device)\n",
    "        y_val = y_val.to(device)\n",
    "\n",
    "        # Perform forward-prop with autograd disabled\n",
    "        with torch.no_grad():\n",
    "            val_out = translator(x_val, y_val)\n",
    "\n",
    "        # Calculate loss\n",
    "        val_out = val_out.permute(0, 2, 1)\n",
    "        batch_loss = loss_fn(val_out[:, :, :-1], y_val[:, 1:])*batch_size\n",
    "        \n",
    "        loss += batch_loss\n",
    "        n_processed += batch_size\n",
    "    \n",
    "    return loss/n_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0985da3cb071adab65c1451180ab2f7",
     "grade": false,
     "grade_id": "cell-da63411703d24d14",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_metrics(fig, ax, ns, train_losses, val_losses):\n",
    "\n",
    "    # Plot losses\n",
    "    ax.clear()\n",
    "    ax.plot(ns, train_losses)\n",
    "    ax.plot(ns, val_losses)\n",
    "    ax.plot(ns, [1]*len(ns), 'k--')\n",
    "    ax.set_title('Loss')\n",
    "    ax.legend(['Train','Validation', 'Validation loss threshold'])\n",
    "    ax.set_xlabel('Number of trained batches')    \n",
    "    ax.grid()\n",
    "    \n",
    "    \n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc5fa6d5b913a0afba5801224524d324",
     "grade": false,
     "grade_id": "cell-3f9b1c062f444a2d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Note that now the validation set metrics are computed using batches (instead of the entire dataset in one go, like we did in task 1) in order to reduce memory usage. It's also noteworthy that we're computing both losses using teacher forcing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e6b74fa61252b67fcd9d10d5c397f3f",
     "grade": false,
     "grade_id": "cell-bd4c43ab07d6c35c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "With these helper functions we can write the train function in a very similar way to task 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0ea7958ff78bd6f00830ac4eb44dbe0",
     "grade": false,
     "grade_id": "cell-23110376805bb695",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "def train(translator, n_epochs, learning_rate, batch_size, train_dataset, val_dataset):\n",
    "    \n",
    "    # Setup the figure for plotting progress during training\n",
    "    %matplotlib notebook\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    plt.ion()\n",
    "    plot_interval = 100\n",
    "    \n",
    "    # Create arrays to average training metrics across batches\n",
    "    losses = []\n",
    "    \n",
    "    # Create dictionaries to hold the computed metrics in\n",
    "    train_data = {'losses': []}\n",
    "    val_data = {'losses': []}\n",
    "    \n",
    "    optimizer = Adam(translator.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    batch_idxs = []\n",
    "    \n",
    "    # Training loop\n",
    "    i_batch = 0\n",
    "    for n in range(n_epochs):\n",
    "        for i, (x_batch, y_batch) in enumerate(train_data_loader):\n",
    "            i_batch += 1\n",
    "            \n",
    "            # Put data in the correct device\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            # Compute loss and outputs\n",
    "            loss = train_batch(translator, x_batch, y_batch, optimizer)\n",
    "            \n",
    "            # Aggregate for later averaging\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Compute metrics and plot after every `plot_interval` batches\n",
    "            if i % plot_interval == 0:\n",
    "              \n",
    "                val_loss = compute_metrics_on_validation_set(translator, val_data_loader)\n",
    "                \n",
    "                val_data['losses'].append(val_loss)\n",
    "                train_data['losses'].append(sum(losses)/len(losses))\n",
    "                batch_idxs.append(i_batch)\n",
    "\n",
    "                losses = []\n",
    "\n",
    "                plot_metrics(fig, ax, batch_idxs, train_data['losses'], val_data['losses'])\n",
    "                \n",
    "    return y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f91cb5336ca4537fbf022992632cd909",
     "grade": false,
     "grade_id": "cell-fd36bbe465cd74d0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "And we can now actually perform the training! Create a `Translator` object in the next cell and train it using the `train` function. Tune the hyper-parameters so that the validation loss ends below 1.\n",
    "\n",
    "**Tips**:\n",
    "- Tuning the hyper-parameters (number of hidden units, learning rate, batch size, etc) will take some trial-and-error. Try simple things first, and then once you manage to train them, start scaling up. Also, have in mind the bias-variance tradeoff mentioned in the lectures.\n",
    "- When tuning the learning rate, focus first on being able to decrease the training loss. Keep decreasing the learning rate until that starts happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e57ce6b72da60df3d7969e83a027ab0a",
     "grade": true,
     "grade_id": "cell-7c18b8ba0338ce9e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "789e99e0c508d5552130347eb345db7b",
     "grade": false,
     "grade_id": "cell-dbcee1678cbfb5d4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Make sure you get the validation loss down to at least 1.0 before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db556efe809c1afc0bb9ad0c998b0663",
     "grade": false,
     "grade_id": "cell-bb406463353d0579",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.5 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35edaf2bc1257eb910c7608a2900e883",
     "grade": false,
     "grade_id": "cell-fd131b12fbda2d16",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we can evaluate how well our translator performs in unseen sentences. The following functions can be used to streamline the translation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dffa9f741679ff11472bb752eca49334",
     "grade": false,
     "grade_id": "cell-c3e14eb95c6c1574",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def tensor2str(t, lang):\n",
    "    # Remove batch dimension\n",
    "    t = t[0]\n",
    "    \n",
    "    # Get numpy array\n",
    "    t = t.cpu().numpy()\n",
    "    \n",
    "    # Map each index to its corresponding word\n",
    "    list_words = list(map(lambda i: lang.index2word[i], t))\n",
    "    \n",
    "    sentence = ' '.join(list_words)\n",
    "    return sentence.replace('PAD', '').replace('EOS', '').replace('SOS', '').strip()\n",
    "\n",
    "def translate_sentence(translator, in_sentence, n_max_preds=10):\n",
    "    # Transform input sentence into its tensor representation (and add batch dimension)\n",
    "    input_tensor = sentence2tensor(input_lang, in_sentence)[None, :].to(device=device)\n",
    "    \n",
    "    # Translate using the learned translator\n",
    "    output_tensor = translator.forward_no_teacher(input_tensor, n_max_preds)\n",
    "    \n",
    "    # Transform back to a sentence\n",
    "    out_sentence = tensor2str(output_tensor, output_lang)\n",
    "    \n",
    "    return out_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "502da6deec5f58ed72c21625133900c5",
     "grade": false,
     "grade_id": "cell-c7b3f86a3d43ceb5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Using them, it's straightforward to translate new sentences from the test set. Note that we're not using teacher forcing anymore, so it's natural to observe some drop in performance (we're also not printing any special word tokens, like `<SOS>`, or `<PAD>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01e5329169254de54a55b4d36537cdfb",
     "grade": false,
     "grade_id": "cell-459f18c4c522873c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n_translations = 10\n",
    "for n in range(n_translations):\n",
    "    i = np.random.randint((len(test_dataset)))\n",
    "    x, y = test_dataset[i]\n",
    "    input_sentence = tensor2str(x[None,:], input_lang)\n",
    "    output_sentence = tensor2str(y[None,:], output_lang)\n",
    "    print('Input:\\t\\t', input_sentence)\n",
    "    print('Translation:\\t', output_sentence)\n",
    "    print('Prediction:\\t', translate_sentence(translator, input_sentence))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32885b7548a528a3717b836ae9619a6f",
     "grade": false,
     "grade_id": "cell-fe8f404a74761ed5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "And we can also translate other sentences we want to (just remember that the input sentence can only contain words from the vocabulary we used to train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_sentence(translator, 'this is a test sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83c62244957792e4614850fa34f65b28",
     "grade": false,
     "grade_id": "cell-09a52180cf6aa5ad",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Finally, in order to get a higher-level view of the performance of your translator across the entire test set, we can compute its [BLEU score](https://en.wikipedia.org/wiki/BLEU). This is a metric whose value ranges from 0 to 1, higher values meaning that the translations are closer to ground-truth. However, only a perfect match between prediction and ground-truth yields a value of 1, so very few translations will be this high. The original [BLEU paper](https://www.aclweb.org/anthology/P02-1040) gives one report of a human baseline for a dataset they used:\n",
    "\n",
    "> (...) on a test corpus of about 500 sentences (40 general news stories), a human translator scored 0.3468 against four references and scored 0.2571 against two references.\n",
    "\n",
    "To compute this metric, we use the `nltk` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a43315e13f0bf963f2a6b09627ccdd89",
     "grade": false,
     "grade_id": "cell-5df1c6895c7e3fc7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "references = []\n",
    "preds = []\n",
    "for i, (x, y) in enumerate(test_dataset):\n",
    "    input_sentence = tensor2str(x[None, :], input_lang)\n",
    "    output_sentence = tensor2str(y[None, :], output_lang)\n",
    "    translated = translate_sentence(translator, input_sentence)   \n",
    "    references.append([output_sentence.split()])\n",
    "    preds.append(translated.split())\n",
    "    \n",
    "    if i % 1000 == 0 and i != 0:\n",
    "        print(i, 'sentences processed...')\n",
    "\n",
    "bleu = corpus_bleu(references, preds)\n",
    "print('Finished! The BLEU score is: {}'.format(bleu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9ca78db00bfad99e7575ed42da16b1b",
     "grade": false,
     "grade_id": "cell-9b25041f9e121f34",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "For us, a value ranging from 0.1 - 0.2 is reasonable (note that performance can be sensitive to the initialization of the network, so it might vary among different runs of the same optimization), specially since we're only comparing to one reference translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e9ca833440393726c6a7d50e284ca64",
     "grade": false,
     "grade_id": "cell-71c02eae358d2a85",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "You have now completed HA2. **Good job!**\n",
    "\n",
    "**Final Question:**\n",
    "What changes could you make to the translator (e.g. architecture, optimization, data used, etc) to improve the end result of your translations? Make three suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a3c6700c06264c4767f64dc8dce42cb",
     "grade": true,
     "grade_id": "cell-1912255711ce6aa8",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
