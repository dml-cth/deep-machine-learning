{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20a4d65ee77906a86bc39bc4046a2a36",
     "grade": false,
     "grade_id": "cell-5690119ead85e67e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Checklist for submission\n",
    "\n",
    "It is extremely important to make sure that:\n",
    "\n",
    "1. Everything runs as expected (no bugs when running cells);\n",
    "2. The output from each cell corresponds to its code (don't change any cell's contents without rerunning it afterwards);\n",
    "3. All outputs are present (don't delete any of the outputs);\n",
    "4. Fill in all the places that say `# YOUR CODE HERE`, or \"**Your answer:** (fill in here)\".\n",
    "5. Never copy/paste any notebook cells. Inserting new cells is allowed, but it should not be necessary.\n",
    "6. The notebook contains some hidden metadata which is important during our grading process. **Make sure not to corrupt any of this metadata!** The metadata may for example be corrupted if you copy/paste any notebook cells, or if you perform an unsuccessful git merge / git pull. It may also be pruned completely if using Google Colab, so watch out for this. Searching for \"nbgrader\" when opening the notebook in a text editor should take you to the important metadata entries.\n",
    "7. Although we will try our very best to avoid this, it may happen that bugs are found after an assignment is released, and that we will push an updated version of the assignment to GitHub. If this happens, it is important that you update to the new version, while making sure the notebook metadata is properly updated as well. The safest way to make sure nothing gets messed up is to start from scratch on a clean updated version of the notebook, copy/pasting your code from the cells of the previous version into the cells of the new version.\n",
    "8. If you need to have multiple parallel versions of this notebook, make sure not to move them to another directory.\n",
    "9. Although not forced to work exclusively in the course `conda` environment, you need to make sure that the notebook will run in that environment, i.e. that you have not added any additional dependencies.\n",
    "\n",
    "**FOR HA1, HA2, HA3 ONLY:** Failing to meet any of these requirements might lead to either a subtraction of POEs (at best) or a request for resubmission (at worst).\n",
    "\n",
    "We advise you to perform the following steps before submission to ensure that requirements 1, 2, and 3 are always met: **Restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). This might require a bit of time, so plan ahead for this (and possibly use a cloud GPU in HA1 and HA2 for this step). Finally press the \"Save and Checkout\" button before handing in, to make sure that all your changes are saved to this .ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6bb874a16c1ff767ac0f37ce0491265",
     "grade": false,
     "grade_id": "cell-774c93bf6433de68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in name of notebook file\n",
    "This might seem silly, but the version check below needs to know the filename of the current notebook, which is not trivial to find out programmatically.\n",
    "\n",
    "You might want to have several parallel versions of the notebook, and it is fine to rename the notebook as long as it stays in the same directory. **However**, if you do rename it, you also need to update its own filename below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_fname = \"XXX.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "879883c2ea755808ffd00aeee5c77a00",
     "grade": false,
     "grade_id": "cell-5676bcf768a7f9be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in group number and member names (use NAME2 and GROUP only for HA1, HA2 and HA3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME1 = \"\" \n",
    "NAME2 = \"\"\n",
    "GROUP = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42f960a95815e1aa3ce8132fcec59cd9",
     "grade": false,
     "grade_id": "cell-a15fe781533d9590",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "736e393ef62f60d5e70432726e7209e0",
     "grade": false,
     "grade_id": "cell-2b9c2390ee464c39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from platform import python_version_tuple\n",
    "assert python_version_tuple()[:2] == ('3','9'), \"You are not running Python 3.9. Make sure to run Python through the course Conda environment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15ec4309f1e85f6e17bda73b9b6f48a2",
     "grade": false,
     "grade_id": "cell-4869b45600ce82f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check that notebook server has access to all required resources, and that notebook has not moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2d199303c73ec86d25177caf39e385f",
     "grade": false,
     "grade_id": "cell-122ac3d9100b8afb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "nb_dirname = os.path.abspath('')\n",
    "assignment_name = os.path.basename(nb_dirname)\n",
    "assert assignment_name in ['IHA1', 'IHA2', 'HA1', 'HA2', 'HA3'], \\\n",
    "    '[ERROR] The notebook appears to have been moved from its original directory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f09f40b5350db83232189137c550f0a1",
     "grade": false,
     "grade_id": "cell-2455deee513cd39c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify correct nb_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a78c7227b049bb147e6c363affb6dae8",
     "grade": false,
     "grade_id": "cell-0472e2fd710f1d72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "try:\n",
    "    display(HTML(r'<script>if(\"{nb_fname}\" != IPython.notebook.notebook_name) {{ alert(\"You have filled in nb_fname = \\\"{nb_fname}\\\", but this does not seem to match the notebook filename \\\"\" + IPython.notebook.notebook_name + \"\\\".\"); }}</script>'.format(nb_fname=nb_fname)))\n",
    "except NameError:\n",
    "    assert False, 'Make sure to fill in the nb_fname variable above!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98d88d8e8da19693053764f29dcc591d",
     "grade": false,
     "grade_id": "cell-ceacb1adcae4783d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify that your notebook is up-to-date and not corrupted in any way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb700528d4644601c1a8c91ef1d84635",
     "grade": false,
     "grade_id": "cell-f5a59288e11b4aec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from ha_utils import check_notebook_uptodate_and_not_corrupted\n",
    "check_notebook_uptodate_and_not_corrupted(nb_dirname, nb_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5285d7fc7207e47d532f63ff1fb7a339",
     "grade": false,
     "grade_id": "cell-3c556a39514d3d6c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# HA2:  Part 2 - Transformers and self-attention\n",
    "$$\n",
    "\\renewcommand{\\vec}[1]{#1}\n",
    "\\def\\x{\\vec{x}}\n",
    "\\def\\y{\\vec{y}}\n",
    "\\def\\dim{d}\n",
    "\\def\\w{W}\n",
    "\\def\\wu{Z}\n",
    "\\def\\R{\\mathbb{R}}\n",
    "\\def\\linMap{W}\n",
    "% Query, key and val\n",
    "\\def\\q{\\vec{q}}\n",
    "\\def\\k{\\vec{k}}\n",
    "\\def\\v{\\vec{v}}\n",
    "\\def\\Wq{\\linMap_Q}\n",
    "\\def\\Wk{\\linMap_K}\n",
    "\\def\\Wv{\\linMap_V}\n",
    "$$\n",
    "*You should have completed part 1 before starting with this one*\n",
    "\n",
    "In this part we will take a closer look at the transformer architecture and the self-attention operation.\n",
    "We will start with basic self-attention and gradually construct an actual self-attention module.\n",
    "Finally we will construct a complete transformer and test it on an actual problem.\n",
    "\n",
    "The focus is on a conceptual understanding of the transformer but you will have to implement a few key elements of a transformer. Along the way we will try to give some best practices for constructing a more complex network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "636b7e895cdc24297839bd0182ee36e0",
     "grade": false,
     "grade_id": "cell-1531dbb9354dac88",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let's start with importing the modules we are going to need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0d6a43981cdc2840d219216030815cc",
     "grade": false,
     "grade_id": "cell-a4bff0f2271fff88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa2e1bfc4f7d31e3912b5158fe949bc7",
     "grade": false,
     "grade_id": "cell-773a1685c568b86b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 1. Basic self-attention\n",
    "\n",
    "The key-stone of the transformer architecture, self-attention, is a sequence-sequence operation which transforms a sequence of input vectors $\\x_1, \\dots \\x_t$ to output vectors $\\y_1, \\dots \\y_t$.\n",
    "Remember that all vectors have the same dimension $\\dim$, i.e. $\\x_i, \\y_i \\in \\R^{\\dim}, \\forall i = 1, \\dots t$.\n",
    "\n",
    "## Weighted average\n",
    "The actual transformation is a simple weighted average\n",
    "$$\n",
    "\\y_i = \\sum_{j} \\x_j \\w_{ji}.\n",
    "$$\n",
    "\n",
    "In an actual transformer, weighted averages are computed often and for long sequences. Therefore, the implementation must be fast in order for training to be even possible.\n",
    "With high-level frameworks such as `pytorch`, the key to fast code is often to reduce loops and instead express computations as matrix operations.\n",
    "\n",
    "**[3 points]** Complete the function snippet below to implement weighted average.\n",
    "\n",
    "In this part of the assignment, one point is given for the correct (but not necessarily efficient) implementation; to get one extra point, you must implement it with just a single for loop, and for two extra points, do it without any loops at all.\n",
    "\n",
    "*Hint*: Take a look at how `torch.bmm` is used later in the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1270106344849b8e7b0d6a415bc98e84",
     "grade": true,
     "grade_id": "cell-f846c494c826a310",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def weighted_avg(x, weights):\n",
    "    \"\"\"Weighted average\n",
    "    Calculates a weighted average of a batch of sequences of vectors.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "        \n",
    "        weights (torch.Tensor): Shape (batch_size, seq_len, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        y (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e61019c60f6cbf50e3f06c1d5e56069b",
     "grade": false,
     "grade_id": "cell-c2fb9d37d6959f02",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Make sure to test your implementation with the unit tests below.\n",
    "The tests cover:\n",
    "\n",
    "1. Dimensionality\n",
    "2. Uniform weights $\\w_{ji} = \\frac{1}{t}$ should produce $y_i: y_i = \\frac{1}{\\dim} \\sum_{j} x_j,\\, \\forall i = 1, \\dots t$\n",
    " (i.e., every $y_i$ is an average of the input sequence).\n",
    "3. A specific numerical example with batch size = 2, $t = 2,\\, \\dim=1$.\n",
    "4. Not breaking gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b284ce8e2056dcb7d06476f2f4a2f3d0",
     "grade": false,
     "grade_id": "cell-1a58d8ec5834578f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def test_weighted_avg(function):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        function: Implementation to test\n",
    "    \"\"\"\n",
    "    # Testing dimension of averaged tensor.\n",
    "    batch_size, dim, seq_len = 5, 2, 3\n",
    "    x = torch.rand(batch_size, dim, seq_len)\n",
    "    weights = torch.rand(batch_size, seq_len, seq_len)\n",
    "    y = function(x, weights)\n",
    "    assert y.shape == (\n",
    "        batch_size,\n",
    "        dim,\n",
    "        seq_len,\n",
    "    ), \"Dimension error: expected y to have shape {}, got {}.\".format(\n",
    "        (batch_size, dim, seq_len), tuple(y.shape)\n",
    "    )\n",
    "\n",
    "    # Testing uniform weights preserve x.\n",
    "    batch_size, dim, seq_len = 5, 2, 3\n",
    "    x = torch.rand(batch_size, dim, seq_len)\n",
    "    weights = torch.ones((batch_size, seq_len, seq_len)).float() / seq_len\n",
    "    y = function(x, weights)\n",
    "    assert all(\n",
    "        torch.allclose(y_b.mean(1), y_b[:, 0]) for y_b in y\n",
    "    ), \"Numerical error: With uniform weights, expected y_i = y_j forall i, j (within each batch).\"\n",
    "    assert all(\n",
    "        torch.allclose(y_b.mean(1), x_b.mean(1)) for (x_b, y_b) in zip(x, y)\n",
    "    ), \"Numerical error: With uniform weights, expected y_i = sum_j x_j, for all i\"\n",
    "\n",
    "    # Actual numerical example.\n",
    "    x = torch.tensor([4, 1]).reshape((1, 1, 2)).float()\n",
    "    unnorm_weights = torch.arange(1, 5).reshape((1, 2, 2)).float()\n",
    "    scale = unnorm_weights.sum(1).reshape((1, 1, 2))\n",
    "    weights = unnorm_weights / scale\n",
    "\n",
    "    y = function(x, weights)\n",
    "    y_true = torch.tensor([7 / 4, 2]).reshape(1, 1, 2).float()\n",
    "    assert torch.allclose(y, y_true), \"Numerical error, expected: {}, got {}\".format(\n",
    "        y_true, y\n",
    "    )\n",
    "\n",
    "    x = torch.rand((batch_size, dim, seq_len), requires_grad=True)\n",
    "    weights = torch.ones((batch_size, seq_len, seq_len), requires_grad=True).float() / seq_len\n",
    "\n",
    "    try:\n",
    "        y = function(x, weights)\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "        print(\"Your function failed to run with the given inputs. Most likely, this is due to breaking gradients.\")\n",
    "\n",
    "    print(\"Test passed.\")\n",
    "\n",
    "\n",
    "test_weighted_avg(function=weighted_avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c063efef59ca2063de9d285bc11e49f",
     "grade": false,
     "grade_id": "cell-06c2f57520370ab9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Defining weights through the dot product\n",
    "A simple way to define $\\w_{ji}$ is with the dot product\n",
    "\n",
    "$$\n",
    "\\wu_{ji} = \\x_j^T \\x_i.\n",
    "$$\n",
    "which maps the pair of input vectors to a non-negative scalar, $\\R^{\\dim \\times \\dim} \\to [0, \\infty)$.\n",
    "We then use a softmax to obtain normalised $\\w_{ji} \\in (0, 1]$:\n",
    "\n",
    "$$\n",
    "\\w_{ji} = \\frac{ e^{\\wu_{ji}} }{ \\sum_j e^{\\wu_{ji}} }.\n",
    "$$\n",
    "\n",
    "**[2 points]** What is the difference between these weights ($W_{ji}$) and the weights in ordinary networks, e.g. a CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4953293866de8e591048d63b29c20127",
     "grade": true,
     "grade_id": "cell-c9cb3ec23bbe5c66",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "247a3bc8a1bd9092465230f0cd6edc38",
     "grade": false,
     "grade_id": "cell-39d421f43d0c02c4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The dot product is essential for calculating the weights. As we progress, we will make slight modifications to the inputs but we will still base it around a function which calculates a softmax-normalized dot product. Therefore, you need to complete the implementation below **[3 points]**:\n",
    "\n",
    "Again, this function will be evaluated often and for long sequences in the transformer block. For all points, implement it without using for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0441d5fd905a16a54372b4b5882ae6c2",
     "grade": true,
     "grade_id": "cell-2a548cd9fdb03d8f",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def normalized_dot_product(v_1, v_2):\n",
    "    \"\"\"Normalized dot products between all pairs of vectors in a sequence\n",
    "    Takes two batches of sequences of vectors as input.\n",
    "    Sequences in the batch are processed independently.\n",
    "    The normalization is done with a softmax function along the columns of the weight matrices.\n",
    "    \n",
    "    Args:\n",
    "        v_1 (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "        v_2 (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "\n",
    "    Returns:\n",
    "        norm_dot_prod (torch.Tensor): Shape (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9ea106f0be219148617203c1b0e8bac",
     "grade": false,
     "grade_id": "cell-9a0498b4a61bc45e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Make sure to test your implementation with the unit tests below.\n",
    "The tests cover:\n",
    "\n",
    "1. Dimensionality\n",
    "2. Normalized in the correct dimension\n",
    "3. A specific numerical example\n",
    "4. Not breaking gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a14d23613a7b697513b02014b137a3cb",
     "grade": false,
     "grade_id": "cell-17aa1999bb0519d6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def test_normalized_dot_product(function):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        function: Implementation to test\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, dim, seq_len = 5, 2, 3\n",
    "    v_1 = torch.rand(batch_size, dim, seq_len)\n",
    "    v_2 = torch.rand(batch_size, dim, seq_len)\n",
    "    weights = function(v_1, v_2)\n",
    "\n",
    "    # Testing dimension of weights.\n",
    "    assert weights.shape == (\n",
    "        batch_size,\n",
    "        seq_len,\n",
    "        seq_len,\n",
    "    ), \"Dimension error: expected weights to have shape {}, got {}.\".format(\n",
    "        (batch_size, seq_len, seq_len), tuple(weights.shape)\n",
    "    )\n",
    "\n",
    "    # Testing weights non-negative\n",
    "    # (Boolean tensor's can be reduced to a single boolean)\n",
    "    assert not (\n",
    "        weights < 0.0\n",
    "    ).any(), \"Value error: expected weights to be non-negative.\"\n",
    "\n",
    "    # Testing weights smaller than one\n",
    "    assert (weights < 1.0).all(), \"Value error: expected weights to be non-negative.\"\n",
    "\n",
    "    assert torch.allclose(\n",
    "        weights.sum(1), torch.ones((batch_size, seq_len))\n",
    "    ), \"ValueError: expected columns (dim 1) to sum to 1.0\"\n",
    "\n",
    "    # Actual numerical example\n",
    "    v_1 = torch.tensor([[1, 2], [-1, 1]]).float().reshape((1, 2, 2))\n",
    "    v_2 = torch.tensor([[1, 0], [1, -1]]).float().reshape((1, 2, 2))\n",
    "    e = np.exp(1)\n",
    "    true_weights = (\n",
    "        torch.tensor(\n",
    "            [\n",
    "                [1 / (e ** 3 + 1), e ** 2 / (e ** 2 + 1)],\n",
    "                [e ** 3 / (e ** 3 + 1), 1 / (e ** 2 + 1)],\n",
    "            ]\n",
    "        )\n",
    "        .float()\n",
    "        .reshape((1, 2, 2))\n",
    "    )\n",
    "    weights = function(v_1, v_2)\n",
    "    assert torch.allclose(\n",
    "        true_weights, weights\n",
    "    ), \"Numerical error: expected {}, got {}.\".format(true_weights, weights)\n",
    "\n",
    "    # Testing gradients\n",
    "    v_1 = torch.rand((batch_size, dim, seq_len), requires_grad=True)\n",
    "    v_2 = torch.rand((batch_size, dim, seq_len), requires_grad=True)\n",
    "    try:\n",
    "        weights = function(v_1, v_2)\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "        print(\"Your function failed to run with the given inputs. Most likely, this is due to breaking gradients.\")\n",
    "\n",
    "    print(\"Test passed.\")\n",
    "\n",
    "\n",
    "test_normalized_dot_product(function=normalized_dot_product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bea90e7cfd9acf3023bd00744e551f9a",
     "grade": false,
     "grade_id": "cell-746bafddcbfd9aba",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "That's it, we have now the building blocks needed for basic self-attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76a3115296294fa75d3c38f05249de59",
     "grade": false,
     "grade_id": "cell-4bb35eab7404cf63",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def basic_self_attention(x):\n",
    "    \"\"\"Basic self-attention\n",
    "    Transforms a batch of sequences of vectors.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        y (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "    \"\"\"\n",
    "    weights = normalized_dot_product(x, x)\n",
    "    return weighted_avg(x, weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d09007f6c93aadee8c10cf83c8096d6f",
     "grade": false,
     "grade_id": "cell-648a50a17d0269e2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 2. A self-attention module\n",
    "Like you saw in the video lectures, self-attention is rarely used in the basic form we have created above.\n",
    "Let's do the modifications needed to construct an actual transformer.\n",
    "\n",
    "We will wrap it in a proper `torch.nn` module to create a building block that we can use in a network.\n",
    "Creating your own module is actually not that common, frameworks like `pytorch` are built to be *modular* and we can often create very specific networks by combining standard modules. That is a good thing, since it enables us to express interesting models in a high-level interface and as a bonus, we build a model from well-tested and efficient parts.\n",
    "With that said, you might find yourself in a situation (perhaps already in the project) where no off-the-shelf module suits your need and you have to create one yourself. View this latter part as an example/inspiration of how to construct a non-trivial custom module.\n",
    "\n",
    "## Queries, keys and values\n",
    "The self-attention is extended with three linear mappings $\\Wq, \\Wk, \\Wv \\in \\R^{\\dim \\times \\dim}$ .\n",
    "These give us learnable parameters and make self-attention more flexible.\n",
    "The three matrices map the input $\\x_i$ into a query, key and value respectively:\n",
    "\n",
    "\\begin{align}\n",
    "    \\q_i = \\Wq \\x_i \\\\\n",
    "    \\k_i = \\Wk \\x_i \\\\\n",
    "    \\v_i = \\Wv \\x_i\n",
    "\\end{align}\n",
    "\n",
    "First, we modify the self-attention by redefining the unnormalized weights (while reusing the notation):\n",
    "\n",
    "\\begin{align}\n",
    "    \\wu_{ji} = \\k_j^T \\q_i \\Big/ \\sqrt{\\dim}\n",
    "\\end{align}\n",
    "The normalized weights are still obtained by applying the softmax function.\n",
    "\n",
    "**[2 points]** Explain why we scale the dot product with the factor $1 / \\sqrt{\\dim}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52f47167a40367ef6b5a2ddead84d8e1",
     "grade": true,
     "grade_id": "cell-d351e9c8c79ee5df",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40ce65b07f6227c29a62533286cf428a",
     "grade": false,
     "grade_id": "cell-eeac94c3ecfcc5e8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Finally, the weighted average is modified and is now based on the values $\\v_j$, instead of on $\\x_j$ directly:\n",
    "$$\n",
    "\\y_i = \\sum_{j} \\v_j \\w_{ji}.\n",
    "$$\n",
    "\n",
    "We can reuse our dot product calculation by simple *wrapping* it in a function that takes queries and keys as the argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dc02868b7a3f70119285481fb452d4b",
     "grade": false,
     "grade_id": "cell-e1459474d3e61f00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def query_key_weights(queries, keys):\n",
    "    \"\"\"Weights from query-key dot product.\n",
    "    Softmax-normalised dot product weights\n",
    "    Calculates weights for a batch of sequences of vectors.\n",
    "    \n",
    "    Args:\n",
    "        queries (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "        keys (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        weights (torch.Tensor): Shape (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    dim = queries.shape[1]\n",
    "    queries = queries / (dim ** (1 / 4))\n",
    "    keys = keys / (dim ** (1 / 4))\n",
    "    return normalized_dot_product(keys, queries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "51b131313af6fd679d3ebc7a835cd665",
     "grade": false,
     "grade_id": "cell-4473215afd0827d8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Multi-head self-attention\n",
    "\n",
    "The model should be able to find different patterns in the input sequence, which is why we use multiple heads.\n",
    "\n",
    "Now, we'll create the actual self-attention function, which includes multiple heads.\n",
    "For implementation simplicity and efficiency, the input vector is split into parts and each attention head is applied to just one part of the vector.\n",
    "Imagine that we have $d = 64$ and four heads, then each head would operate on a vector with dimension $64 / 4 = 16$.\n",
    "\n",
    "## Constructing the module\n",
    "Below is an implementation of our self-attention module. We try to show you how a typical custom model looks like. Part of that is to do full vectorization (i.e. no loops). The result is a lot of manipulation of shapes and dimension order of intermediate tensors. It might be quite difficult to read and to wrap your head around, but since you are likely to use and modify other peoples code (in the project or some later time), it is good that you get exposed to it now.\n",
    "\n",
    "We also use a package called ``einops`` which makes it easier to manipulate dimensions of tensors. It is not necessary to use it, but it makes the code a bit more readable. Please run the cell below to install `einops` if you haven't done so already. Also, make sure you have activated the `dml` environment before running the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9cf0ffec945403f20b9b2e225e170ae",
     "grade": false,
     "grade_id": "cell-661482850f134902",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c02515a38708bf031281a511824ef94b",
     "grade": false,
     "grade_id": "cell-535e46dd571b9284",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim, heads):\n",
    "        \"\"\"Self-attention module\n",
    "\n",
    "        Args:\n",
    "            dim (int): The full embedding dimension of the input vectors\n",
    "            heads (int): The number of heads in the multi-head attention.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if not dim % heads == 0:\n",
    "            raise ValueError(\n",
    "                \"The embedding dim. must be divisible by the number of heads for the vectorization to work.\"\n",
    "            )\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        head_dim = dim // heads\n",
    "        # Linear maps for q, k and v\n",
    "        self.Wq = nn.Linear(dim, heads * head_dim, bias=False)\n",
    "        self.Wk = nn.Linear(dim, heads * head_dim, bias=False)\n",
    "        self.Wv = nn.Linear(dim, heads * head_dim, bias=False)\n",
    "        # Linear mapping to return to the original\n",
    "        self.WO = nn.Linear(heads * head_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Multi-headed self attention\n",
    "\n",
    "        Each head operates on a part of the embedding, i.e. we have q, k and v with shape\n",
    "        (batch_size, seq_length, heads, dim / heads)\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input with shape (batch_size, seq_length, dim)\n",
    "        \"\"\"\n",
    "\n",
    "        # The input is projected with the three different linear maps\n",
    "        # and reshaped to combine the batch and head dimensions for efficiency.\n",
    "        keys = rearrange(self.Wk(x), \"b s (h d) -> (b h) d s\", h=self.heads)\n",
    "        queries = rearrange(self.Wq(x), \"b s (h d) -> (b h) d s\", h=self.heads)\n",
    "        values = rearrange(self.Wv(x), \"b s (h d) -> (b h) d s\", h=self.heads)\n",
    "\n",
    "        # The weights are calculated from the query-key dot product\n",
    "        weights = query_key_weights(queries, keys)\n",
    "\n",
    "        # The output is the weighted average of the values\n",
    "        y_tilde = weighted_avg(values, weights)\n",
    "\n",
    "        # The output is reshaped to a suitable shape before projected back to the original dimension\n",
    "        y_tilde = rearrange(y_tilde, \"(b h) d s -> b s (h d)\", h=self.heads)\n",
    "\n",
    "        return self.WO(y_tilde), weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1a1cebeb15a37888ce9cf8c421a9270",
     "grade": false,
     "grade_id": "cell-c6938740fc6cf9db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# The transformer block\n",
    "\n",
    "The majority of the implementation complexity is actually in the `SelfAttention` module. The transformer block is rather straight forward, it is just like the one described in the video lectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "869ab065fd407a952e62c115511affda",
     "grade": false,
     "grade_id": "cell-9b67db79adeb75d3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block\"\"\"\n",
    "\n",
    "    def __init__(self, dim, heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = SelfAttention(dim, heads)\n",
    "\n",
    "        self.normalization_1 = nn.LayerNorm(dim)\n",
    "        self.normalization_2 = nn.LayerNorm(dim)\n",
    "\n",
    "        # The size of the hidden layer is a hyper-parameter,\n",
    "        # but the consensus is that it should at least be larger than the input/output size\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim, 4 * dim), nn.ReLU(), nn.Linear(4 * dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of Transformer block\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input with shape (batch_size, seq_length, dim)\n",
    "            \n",
    "        Returns:\n",
    "            y (torch.Tensor): Output with shape (batch_size, seq_length, dim)\n",
    "            attention_weights (torch.Tensor): Weights for the attention layer with shape (batch_size, seq_length, seq_length)\n",
    "        \"\"\"\n",
    "        y, attention_weights = self.self_attention(x)\n",
    "        # Note how the residual (skip) connections are implemented as simple addition.\n",
    "        x = self.normalization_1(x + y)\n",
    "        fed_forward = self.feed_forward(x)\n",
    "        return self.normalization_2(fed_forward + x), attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9800f2211ff3b5b80a0e2bae1fa04b4",
     "grade": false,
     "grade_id": "cell-a68000a248c4d4f7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now, we are done with the general module. To create an actual transformer yet we must choose an actual problem so that we can specify input, embedding and output.\n",
    "Let's do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4863b84d58cdbb59749cad4b5e15e3e",
     "grade": false,
     "grade_id": "cell-5fcb1a036ccf47fc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 3. IMDB Classification\n",
    "\n",
    "In this task we will consider classification of IMDB reviews. For pedagogical reasons, the classification will be based upon the functions you have built above. The idea is to clarify the connection between these functions and an application with real data. However, to get better performance, we will rely on a pre-trained NLP model, namely [distill-bert](https://huggingface.co/distilbert-base-uncased) for extracting features and improving performance. This is a smaller version of BERT, a Transformer-based language model that is pretrained on a large corpus and useful for extracting features from text. This is similar to the transfer learning done in HA1 where we utilize powerful, general purpose models for extracting features, and finetune a smaller model for the task at hand.\n",
    "\n",
    "The purpose here is to build on the computer labs and to give you some inspiration for how to solve a general problem with `pytorch`. It will show you how to install additional python libraries (useful for the project) and some advice on how to construct a training/validation loop. We do not expect you to modify the code, **you don't even have to run the training** if you feel that your cloud credits are starting to run low. However, you should read and understand the code; it will help you answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b2b2adfb46cd461a1f83d197fd514bf",
     "grade": false,
     "grade_id": "cell-f4e372ca663783c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Installing additional packages\n",
    "DistilBERT is implemented by an external python module called `transformers` while the IMDB data is provided by `datasets`. Both these can be installed by running the cell below.\n",
    "\n",
    "Make sure that you have activated the dml environment before your run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a05ea264c19633b2f3c545a79a8ff9e0",
     "grade": false,
     "grade_id": "cell-5bde603e8cc786eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b83cab7ce2a884c50f4175632fc02b2",
     "grade": false,
     "grade_id": "cell-782a05ee9fec69cb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## The data\n",
    "Processing text data can be tedious and error prone. For prototyping it is nice to use some third-party library which has done most of the work for you. You do not really need to focus on the data processing here, since it will be different for every task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3e67bc21cd531fec18672754453c676",
     "grade": false,
     "grade_id": "cell-5554838bcc3faced",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "\n",
    "def load_imdb_dataset(model_name, tokenizer):\n",
    "    \"\"\"Loads the IMDB dataset\"\"\"\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    imdb = load_dataset(\"imdb\")\n",
    "\n",
    "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
    "    return tokenized_imdb\n",
    "\n",
    "\n",
    "def get_collator(model_name, tokenizer):\n",
    "    \"\"\"Returns a collator, which is a function that takes a batch of examples and returns a batch of tensors\"\"\"\n",
    "    return DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def get_loaders(batch_size, model_name):\n",
    "    \"\"\"Returns data loaders\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    dataset = load_imdb_dataset(model_name, tokenizer)\n",
    "    data_collator = get_collator(model_name, tokenizer)\n",
    "    dataset.set_format(\"torch\", columns=[\"input_ids\", \"label\", \"attention_mask\"])\n",
    "    train_loader = DataLoader(\n",
    "        dataset[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=data_collator\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset[\"test\"], batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a06af0041a4172e706173436ac7d9c5f",
     "grade": false,
     "grade_id": "cell-e5eaae7beb3dfd1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can have a look at different examples from the dataset. The tokenizer has mapped the text to a sequence of integers, `input_ids`. During this mapping, the tokenizer also adds special tokens, such as padding and a class token (`input_id` 101). The class token is often used for sequence-level classification. \n",
    "\n",
    "Further we can see that the label is represented as 0 (negative review) or 1 (positive review)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_example_text(dataset, index, split=\"train\"):\n",
    "    \"\"\"View an example text from the dataset\"\"\"\n",
    "    print(dataset[split][\"label\"][index])\n",
    "    print(dataset[split][\"text\"][index])\n",
    "    print(dataset[split][\"input_ids\"][index])\n",
    "    print()\n",
    "\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "imdb = load_imdb_dataset(MODEL_NAME, AutoTokenizer.from_pretrained(MODEL_NAME))\n",
    "view_example_text(imdb, 1337)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e2e49e19aafe41b69af71fb05dd4f83",
     "grade": false,
     "grade_id": "cell-d1ffc03b003a1689",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## The transformer\n",
    "We will create a simple transformer that takes as input the processed feature vectors from distilBERT and which outputs a probability vector over the two classes \"neg\" and \"pos\" (technically, the output will be the input to a log-softmax). The model also outputs the attention weights for visualization later.\n",
    "We make the simplest (and less memory efficient) version of position embedding as described in the video lectures.\n",
    "\n",
    "We'll also create a module that holds the pre-trained distilBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "257b5ec029f1f8c6d9ddc2371bdb660c",
     "grade": false,
     "grade_id": "cell-ff83666a724761f0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, in_dim, dim, heads, depth, seq_length, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_map = nn.Linear(in_dim, dim)\n",
    "        self.pos_emb = nn.Embedding(seq_length, dim)\n",
    "        self.max_seq_length = seq_length\n",
    "\n",
    "        transformer_blocks = []\n",
    "        for _ in range(depth):\n",
    "            transformer_blocks.append(TransformerBlock(dim=dim, heads=heads))\n",
    "\n",
    "        # A ModuleList is a list of Modules, so you can access intermediate values and it registers part of the model properly.\n",
    "        # This ensures that all parameters are moved to the correct device when calling Transformer().to(device)\n",
    "        self.transformer_blocks = nn.ModuleList(transformer_blocks)\n",
    "\n",
    "        # The last part is problem specific. Here we want to map our transformer embeddings\n",
    "        # to a probability distribution.\n",
    "        # We will use a linear layer to produce logits (the input to a log-softmax function).\n",
    "        self.output_map = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Transformer forward method\n",
    "\n",
    "        Args:\n",
    "            x Tensor(batch_size, seq_length, in_dim): Input tensor\n",
    "        Returns:\n",
    "            Tensor(batch_size, num_classes): Logits\n",
    "            Tensor(batch_size*num_heads, seq_length, seq_length): Attention weights\n",
    "        \"\"\"\n",
    "        x = self.input_map(x)\n",
    "        batch_size, seq_length, dim = x.shape\n",
    "        if seq_length > self.max_seq_length:\n",
    "            x = x[:, : self.seq_length]\n",
    "\n",
    "        # Note that we create a completely new tensor which must be moved to the proper device.\n",
    "        pos = torch.arange(seq_length, device=x.device)\n",
    "        # Unsqueeze to add the batch dimension, same as [None, ...]\n",
    "        pos = self.pos_emb(pos).unsqueeze(0)\n",
    "        x = x + pos\n",
    "\n",
    "        attention_weights = []\n",
    "\n",
    "        for i, block in enumerate(self.transformer_blocks):\n",
    "            x, attention_weight = block(x)\n",
    "            attention_weights.append(attention_weight.unsqueeze(1))\n",
    "\n",
    "        # Use [CLS] token for classification\n",
    "        x = self.output_map(x[:, 0, :])\n",
    "        return F.log_softmax(x, dim=1), torch.cat(attention_weights, dim=1)\n",
    "\n",
    "\n",
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\"):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.output_size = self.bert.config.hidden_size\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"Forward pass of BERT encoder\n",
    "        \n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Input with shape (batch_size, seq_length)\n",
    "            attention_mask (torch.Tensor): Mask with shape (batch_size, seq_length)\n",
    "\n",
    "        Returns:\n",
    "            Tensor(batch_size, seq_length, hidden_size): Encoded input\n",
    "            Tensor(batch_size, num_layers, seq_length, seq_length): Attention weights\n",
    "        \"\"\"\n",
    "        out = self.bert(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, output_attentions=True\n",
    "        )\n",
    "        return out.last_hidden_state, out.attentions\n",
    "\n",
    "    def to(self, device):\n",
    "        self.bert.to(device)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d83a604f8042da256c6d8cf13c1e2bf9",
     "grade": false,
     "grade_id": "cell-d6bb31f6cd70602b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's combine the two modules into a single network. We'll freeze all layers in the pre-trained model and only train our Transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0e5046f895333f324de327975f71eb6",
     "grade": false,
     "grade_id": "cell-b6726c97e091fb17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ImdbClassifier(nn.Module):\n",
    "    def __init__(self, dim, heads, depth, seq_length, num_classes, freeze_encoder=True):\n",
    "        super().__init__()\n",
    "        self.bert = BERTEncoder()\n",
    "        if freeze_encoder:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            self.bert.output_size, dim, heads, depth, seq_length, num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, *args, **kwargs):\n",
    "        x, bert_attn = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x, transformer_attn = self.transformer(x)\n",
    "        return (\n",
    "            x,\n",
    "            {\"bert_attn\": torch.cat(bert_attn), \"transformer_attn\": transformer_attn},\n",
    "        )\n",
    "\n",
    "    def to(self, device):\n",
    "        self.bert.to(device)\n",
    "        self.transformer.to(device)\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "292f3af0d4ef17d38b1a1fe5fbb348ec",
     "grade": false,
     "grade_id": "cell-957262dc380f1c7c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now, for the train./val loop. This can be written in many ways but based on common misstakes in HA1, hints might be in order:\n",
    "\n",
    "- Separate your code into smaller pieces, i.e. functions. It makes it easier to find bugs and easier to reuse code.\n",
    "- Use separate functions to calculate metrics. If you want to calculate, say accuracy, during both training and validation, don't copy the code. Write one function and make sure that it works, then reuse it.\n",
    "- Adding measurements to a running metrics can be tricky. Below is a solution that is a bit overkill but that is okay, since it is hard to use it incorrectly.\n",
    "\n",
    "Note: the code below can be modified so that you can play around with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, max_seq_len, device):\n",
    "    \"\"\"Train epoch\"\"\"\n",
    "    train_loss = AccumulatingMetric()\n",
    "    train_acc = AccumulatingMetric()\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        for key, val in batch.items():\n",
    "            batch[key] = val.to(device)\n",
    "        label = batch[\"labels\"]\n",
    "        pred, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "        loss = F.nll_loss(pred, label)\n",
    "        loss.backward()\n",
    "        train_loss.add(loss.item())\n",
    "\n",
    "        train_acc.add(accuracy(pred, label))\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    return train_loss.avg(), train_acc.avg()\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, max_seq_len, device):\n",
    "    \"\"\"Validate epoch\"\"\"\n",
    "    val_loss = AccumulatingMetric()\n",
    "    val_acc = AccumulatingMetric()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            for key, val in batch.items():\n",
    "                batch[key] = val.to(device)\n",
    "            label = batch[\"labels\"]\n",
    "            pred, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "            val_loss.add(F.nll_loss(pred, label).item())\n",
    "\n",
    "            val_acc.add(accuracy(pred, label))\n",
    "\n",
    "    return val_loss.avg(), val_acc.avg()\n",
    "\n",
    "\n",
    "def predict_epoch(model, loader, max_seq_len, device):\n",
    "    \"\"\"Run prediction\"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            for key, val in batch.items():\n",
    "                batch[key] = val.to(device)\n",
    "            label = batch[\"labels\"]\n",
    "            pred, _ = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "            hard_pred = pred.argmax(1)\n",
    "            preds.append(hard_pred.cpu())\n",
    "            labels.append(label.cpu())\n",
    "\n",
    "    return torch.cat(preds), torch.cat(labels)\n",
    "\n",
    "\n",
    "def accuracy(pred, label):\n",
    "    hard_pred = pred.argmax(1)\n",
    "    return (hard_pred == label).float().mean().item()\n",
    "\n",
    "\n",
    "def _truncate_input(input_, max_seq_len):\n",
    "    if input_.size(1) > max_seq_len:\n",
    "        input_ = input_[:, :max_seq_len]\n",
    "    return input_\n",
    "\n",
    "\n",
    "class AccumulatingMetric:\n",
    "    \"\"\"Accumulate samples of a metric and automatically keep track of the number of samples.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metric = 0.0\n",
    "        self.counter = 0\n",
    "\n",
    "    def add(self, value):\n",
    "        self.metric += value\n",
    "        self.counter += 1\n",
    "\n",
    "    def avg(self):\n",
    "        return self.metric / self.counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c7a27c9b05fc73dbcffde853e42c869",
     "grade": false,
     "grade_id": "cell-830c84607a25bd32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The code below sets a few hyperparameters and runs the training. If you are having trouble accessing GPUs, feel free to only study the code. We provide a pre-trained model you can load in the next cell.\n",
    "\n",
    "If you want, feel free to play around with the training, or the model parameters. For reference, the provided checkpoint reaches 91% accuracy on the test data. Also, timing this on Azure, a single epoch took ~12 min. \n",
    "\n",
    "**Note**: You have to change the value of `num_epochs`. It is set to 0 initially to avoid training if you run the notebook top to bottom. We trained our model for 5 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "batch_size = 4\n",
    "train_loader, test_loader = get_loaders(batch_size, MODEL_NAME)\n",
    "\n",
    "\n",
    "max_length = 512\n",
    "embedding_size = 128\n",
    "num_heads = 8\n",
    "num_classes = 2\n",
    "depth = 2\n",
    "\n",
    "model = ImdbClassifier(\n",
    "    dim=embedding_size,\n",
    "    heads=num_heads,\n",
    "    depth=depth,\n",
    "    seq_length=max_length,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "lr = 1e-4\n",
    "lr_warmup = 1e4\n",
    "num_epochs = (\n",
    "    0  # Change this to train for more epochs, we trained our model for 5 epochs\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "# A scheduler is a principled way of controlling (often decreasing) the learning rate as time progresses.\n",
    "# Read more: https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lambda i: min(i / (lr_warmup / batch_size), 1.0)\n",
    ")\n",
    "\n",
    "print(\"Starting training\")\n",
    "for epoch in range(num_epochs):\n",
    "    start = time()\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, optimizer, scheduler, max_length, device\n",
    "    )\n",
    "    val_loss, val_acc = validate_epoch(model, test_loader, max_length, device)\n",
    "    end = time()\n",
    "    print(\n",
    "        \"Epoch: {}/{}: time: {:.1f}, train loss: {:.3f}, train acc: {:.3f}, val. loss {:.3f}, val. acc: {:.3f}\".format(\n",
    "            epoch + 1, num_epochs, end - start, train_loss, train_acc, val_loss, val_acc\n",
    "        )\n",
    "    )\n",
    "print(\"You have now trained a transformer!\")\n",
    "\n",
    "# Save all parameters except for the BERT ones\n",
    "reduced_state_dict = {}\n",
    "for key, val in model.state_dict().items():\n",
    "    if key.startswith(\"bert\"):\n",
    "        continue\n",
    "    reduced_state_dict[key] = val\n",
    "torch.save(reduced_state_dict, \"my_model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd99858d2c071e363dc9051223771d10",
     "grade": false,
     "grade_id": "cell-1afc742048509a8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to load the provided checkpoint. **Do not** modify hyperparameters below for loading the provided model. If you have trained your own model, simply skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_loader, test_loader = get_loaders(batch_size, MODEL_NAME)\n",
    "\n",
    "max_length = 512\n",
    "embedding_size = 128\n",
    "num_heads = 8\n",
    "num_classes = 2\n",
    "depth = 2\n",
    "\n",
    "model = ImdbClassifier(\n",
    "    dim=embedding_size,\n",
    "    heads=num_heads,\n",
    "    depth=depth,\n",
    "    seq_length=max_length,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\"model.pt\", map_location=torch.device(\"cpu\")), strict=False\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17f2fa4d7d6fb29af009effff270c94a",
     "grade": false,
     "grade_id": "cell-068e63bc03450ba1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Analysing the model and self-attention\n",
    "Self-attention is a key component of the Transformer. Further, the attention weights can offer some insights into the inner workings of the model, e.g., how the attention weights are used to focus on certain parts of the input. They can also highlight how inputs relate to each other. Of course, one cannot draw conclusions about a model from the attention weights alone, but it still interesting to have a look at them.\n",
    "\n",
    "Below we define some helper functions to visualize the attention weights.\n",
    "\n",
    "**[2 points]** Why is it that we cannot simply inspect the attention weights $W_{ji}$ and draw conclusion about the inner workings and reasoning of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "464615f9038e302f8ea84e1a02b41696",
     "grade": true,
     "grade_id": "cell-f466bbf907a97996",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b48bd8b64777939801de89cbdc30d829",
     "grade": false,
     "grade_id": "cell-1e529a696ddf2289",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def visualize_attention_one_token(attention_weights, input_, token_name=\"\"):\n",
    "    \"\"\"Visualize attention from one token to other tokens for each head.\"\"\"\n",
    "\n",
    "    tokens = input_[\"tokens\"]\n",
    "    num_tokens = len(tokens)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, len(tokens) / 5))\n",
    "\n",
    "    # We remove the padding tokens from the attention weights.\n",
    "    attention_weights = attention_weights[:, :num_tokens]\n",
    "    attention_weights = attention_weights.cpu().numpy()\n",
    "\n",
    "    cmap = sns.heatmap(attention_weights.T, cmap=\"Reds\", yticklabels=tokens)\n",
    "    plt.title(f\"Attention weights for {token_name}\")\n",
    "    plt.xlabel(\"Head index\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_attention_map(attention_weights, input_, title=\"\"):\n",
    "    \"\"\"Visualize attention between all tokens.\"\"\"\n",
    "\n",
    "    tokens = input_[\"tokens\"]\n",
    "    num_tokens = len(tokens)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(len(tokens) / 5, len(tokens) / 5))\n",
    "\n",
    "    # We remove the padding tokens from the attention weights.\n",
    "    attention_weights = attention_weights[:num_tokens, :num_tokens]\n",
    "    attention_weights = attention_weights.cpu().numpy()\n",
    "\n",
    "    cmap = sns.heatmap(\n",
    "        attention_weights.T, cmap=\"Reds\", yticklabels=tokens, xticklabels=tokens,\n",
    "    )\n",
    "    plt.ylabel(\"Queries\")\n",
    "    plt.xlabel(\"Keys\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_attention_plots(\n",
    "    model, input_, lvl_idx=-1, head_idx=0, attn_key=\"transformer_attn\"\n",
    "):\n",
    "    \"\"\"Create attention plots.\"\"\"\n",
    "    # Create attention plots for all tokens in input.\n",
    "    pred, attention_weights = model(input_[\"input_ids\"], input_[\"attention_mask\"])\n",
    "    attention_weights = attention_weights[attn_key]\n",
    "    # Visualize attention from CLS token to other tokens for each head.\n",
    "    if attn_key == \"transformer_attn\":\n",
    "        attention_from_cls_token = attention_weights[:, lvl_idx, :, 0]\n",
    "    elif attn_key == \"bert_attn\":\n",
    "        # Bert attention weights are stored in a different format.\n",
    "        attention_from_cls_token = attention_weights[lvl_idx, :, 0, :]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown attention key: {attn_key}\")\n",
    "\n",
    "    visualize_attention_one_token(attention_from_cls_token, input_, \"[CLS]\")\n",
    "\n",
    "    # Visualize attention between all tokens.\n",
    "    if attn_key == \"transformer_attn\":\n",
    "        attention_weights_head = attention_weights[head_idx, lvl_idx]\n",
    "    elif attn_key == \"bert_attn\":\n",
    "        # Bert attention weights are stored in a different format.\n",
    "        attention_weights_head = attention_weights[lvl_idx, head_idx].T\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown attention key: {attn_key}\")\n",
    "\n",
    "    title = f\"Attention weights for head {head_idx}\"\n",
    "    visualize_attention_map(attention_weights_head, input_, title)\n",
    "\n",
    "\n",
    "def text_to_tensor(text, tokenizer):\n",
    "    \"\"\"Convert text to tensor.\"\"\"\n",
    "    encoded_input = tokenizer(text, truncation=True)\n",
    "    return encoded_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b2754ccb839815f3ced4c169c309581",
     "grade": false,
     "grade_id": "cell-38b42a308fd9ca1d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below we load some of the validation data, send it through the model and visualize the attention weights. The `idx` determines which text from the validation set to use. The `lvl_idx` determines which Transformer block to visualize, 0 corresponds to the first Transformer block, `depth-1` is the last Transformer block. The `head_idx` determines which attention head to visualize.\n",
    "\n",
    "The first figure shows the attention weights from the class token to all other tokens for all heads in the selected `lvl_idx`. The second figure shows the attention weights between all tokens for the selected `head_idx` in the selected `lvl_idx`. The y-axis corresponds to queries while the x-axis corresponds to keys. For longer sequences, the second figure can be hard to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63182aaf2d62ce7de4d91204f5f10e87",
     "grade": false,
     "grade_id": "cell-f3c946ac88313547",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We'll need the tokenizer to convert the text into tensors and vice versa.\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 42\n",
    "lvl_idx = 0\n",
    "head_idx = 0\n",
    "\n",
    "\n",
    "def input_to_tensor(_input, device):\n",
    "    _input[\"tokens\"] = tokenizer.convert_ids_to_tokens(_input[\"input_ids\"])\n",
    "    _input[\"input_ids\"] = torch.tensor(\n",
    "        _input[\"input_ids\"], device=device, dtype=torch.long\n",
    "    ).reshape(1, -1)\n",
    "    _input[\"attention_mask\"] = torch.tensor(\n",
    "        _input[\"attention_mask\"], device=device, dtype=torch.long\n",
    "    ).reshape(1, -1)\n",
    "    _input[\"label\"] = torch.tensor(_input[\"label\"], device=device).reshape(1, -1)\n",
    "    return _input\n",
    "\n",
    "\n",
    "input_ = input_to_tensor(imdb[\"test\"][idx], device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred, _ = model(input_[\"input_ids\"], input_[\"attention_mask\"])\n",
    "    print(f\"Predicted probabilities: {pred.exp()}\")\n",
    "    create_attention_plots(model, input_, lvl_idx=lvl_idx, head_idx=head_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7bfaa6762f422c725e391e2e6869385",
     "grade": false,
     "grade_id": "cell-e90693716f53095f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can also test the model on custom inputs. Simply change the `text` to your own text. Play around to find out what makes the model think a review is positive or negative. Try out neutral reviews to see how the model is able to handle them.\n",
    "\n",
    "Also, try changing the `lvl_idx` and `head_idx` to see how the attention weights change. For looking at the attention weight within the distilBERT model, you can set `attn_key=\"bert_attn\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i do not have an opinion about the movie. i spend most of my time training neural networks, but my friends say it's 'meh'.\"\n",
    "text = \"i love all movies with transformers in them, this one did not have any\"\n",
    "lvl_idx = 0\n",
    "head_idx = 4\n",
    "\n",
    "input_ = text_to_tensor(text, tokenizer)\n",
    "# This does not really matter, but for consistency, we add a label here.\n",
    "input_[\"label\"] = [1]\n",
    "\n",
    "input_ = input_to_tensor(input_, device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred, _ = model(input_[\"input_ids\"], input_[\"attention_mask\"])\n",
    "    print(f\"Predicted probabilities: {pred.exp()}\")\n",
    "    create_attention_plots(\n",
    "        model, input_, lvl_idx=lvl_idx, head_idx=head_idx, attn_key=\"transformer_attn\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25c8b6b0d68ee335400a2056183ec65b",
     "grade": false,
     "grade_id": "cell-1b398c933cf4f655",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[2 points]** The above implementation of the transformer is using multi-head self-attention. Briefly describe which parts are different in different heads of the multi-head self-attention layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbe2791a05da60718f2847a0279bbe34",
     "grade": true,
     "grade_id": "cell-47d1c271915af4da",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1bf99578b7873fe41284447fe025bce5",
     "grade": false,
     "grade_id": "cell-21c3baa58b59eb79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[2 points]** What are the benefits of using multi-head self-attention compared to a single-head self-attention block? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d3990aee9ec890dc76dff1adec61633",
     "grade": true,
     "grade_id": "cell-4613db50a7a6040d",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f99d75df5945365379344bc53c98bbec",
     "grade": false,
     "grade_id": "cell-818a767a6db1d3fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[2 points]** When applying transformers it is common to have a maximum sequence length. What problems can you foresee when applying transformers to longer sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c308c09c69bbe89d4e0e75d6ae193a64",
     "grade": true,
     "grade_id": "cell-f912403f0ed83bd5",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5d154a810c2ca850204acf31865fee3",
     "grade": false,
     "grade_id": "cell-07c29d57cf033982",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[3 points]** In natural language processing, as well as other fields such as computer vision, Transformers have become extremely popular and started replacing standard models such as CNN and RNN. \n",
    "\n",
    "What are the main differences between a transformer and an RNN? In particular, how do the differences make it easier to train a transformer, compared to an RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d91c4edbba3e55f539db5fa6dd7a4f0b",
     "grade": true,
     "grade_id": "cell-f23a746ff66c77ce",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae0003b7708c2132190f18040c94027d",
     "grade": false,
     "grade_id": "cell-e7fe4f876c7d277c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Learned token embedding\n",
    "We can also inspect the learned embeddings of the tokens for the distilBERT model. Each token is mapped to a learned vector before being processed by the Transformer. We can list the embeddings and corresponding tokens with largest cosine similarity to a given token. \n",
    "\n",
    "Try different words to get an intuition for how this works. Interesting examples can be \"good\", \"four\", \"actor\", or words with multiple meanings such as \"feet\" or \"watch\". Are the results what you'd expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "word = \"yes\"\n",
    "topk = 10\n",
    "token_embeddings = model.bert.bert.embeddings.word_embeddings.weight\n",
    "\n",
    "# The first token will be the CLS token, while the last token will be the SEP token. We ignore these tokens.\n",
    "tokenized_word = tokenizer(word, truncation=True).input_ids[1:-1]\n",
    "# Use mean in case word consists of multiple tokens.\n",
    "word_embedding = token_embeddings[tokenized_word].mean(dim=0, keepdim=True)\n",
    "\n",
    "similarity = F.cosine_similarity(word_embedding, token_embeddings, dim=1)\n",
    "similarity = similarity.cpu()\n",
    "\n",
    "_, idx = torch.topk(similarity, k=topk)\n",
    "\n",
    "print(f\"{topk} most similar words to {word}\")\n",
    "for i in range(topk):\n",
    "    print(f\"{i}. {tokenizer.convert_ids_to_tokens(idx[i].item())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points]** The word embeddings seem to capture the semantic meaning of the words in a sense that they are more similar to synonyms than other words in the vocabulary. If this is the case, why do we need to use advanced networks such as Transformers to process them further for classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bce67611711fe46f425f9e461db76f12",
     "grade": true,
     "grade_id": "cell-93c65d0f298f25e2",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "231fa79e77562a2159280cf20f58cb99",
     "grade": false,
     "grade_id": "cell-5fa90e0d65d344ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can also inspect the positional embeddings of the distilBERT model. Below we draw a heatmap for the cosine similarity between the learned positional embeddings. We only draw this for the first `first_k` tokens, feel free to inspect other ranges as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_k = 128\n",
    "\n",
    "pos_embeddings = model.bert.bert.embeddings.position_embeddings.weight\n",
    "sims = pos_embeddings @ pos_embeddings.T\n",
    "pos_emb_norm = torch.norm(pos_embeddings, dim=1)\n",
    "sims = sims / (pos_emb_norm.unsqueeze(1) @ pos_emb_norm.unsqueeze(0))\n",
    "\n",
    "sns.heatmap(sims.cpu().detach().numpy()[:first_k, :first_k], cmap=\"Reds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e38d8cc6556f69bca8829edbeab75a72",
     "grade": false,
     "grade_id": "cell-5e903d8720e0929f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[4 points]** When training the RNN in part 1, we did not need any positional embeddings. \n",
    "\n",
    "Why do we need them now? What would happen if we did not use them? Also, are there any applications where you do not need them when using a Transformer (please be specific here)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b30c3911a8dc10a4a896f6e2fe6cb1f5",
     "grade": true,
     "grade_id": "cell-99986d44df3e5a37",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ba47c464384ee27337bbe494249e6be",
     "grade": false,
     "grade_id": "cell-fad9c5d6f164ad56",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[2 points]** Why do we see these particular patterns in the heatmap above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0c85734426bd8ded8681d2cd610f3df",
     "grade": true,
     "grade_id": "cell-8c5bac786a7bd2fb",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a51d36129eef0c0dbc801974df91b444",
     "grade": false,
     "grade_id": "cell-fcb187adf97b8080",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[1 point]** The first positional embedding (the one for the [CLS] token) has a different pattern than the other embeddings. Why? You can think of the implications in the Transformer if this wasn't the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d876c702d51114121ada8632f4c7df3f",
     "grade": true,
     "grade_id": "cell-87277612f890e674",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09c94fdea543e09bd55225a780f4090b",
     "grade": false,
     "grade_id": "cell-08639295de373894",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Wrapping up\n",
    "\n",
    "The transformer architecture has become incredibly popular and has produced truly amazing results.\n",
    "You should now have a good insight for how they can be implemented in `pytorch`. If you are interested, here are some more resources:\n",
    "\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "- https://github.com/huggingface/transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "d94bb5ce8f1aac59beba206cd685a74cc957e1538cbbfce7cec92ea121e47d25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
