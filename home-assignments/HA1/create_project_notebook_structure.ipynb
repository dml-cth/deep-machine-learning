{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "752179840c6f3d2e50289f59b79a8101",
     "grade": false,
     "grade_id": "cell-5690119ead85e67e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Checklist for submission\n",
    "\n",
    "It is extremely important to make sure that:\n",
    "\n",
    "1. Everything runs as expected (no bugs when running cells);\n",
    "2. The output from each cell corresponds to its code (don't change any cell's contents without rerunning it afterwards);\n",
    "3. All outputs are present (don't delete any of the outputs);\n",
    "4. Fill in all the places that say `# YOUR CODE HERE`, or \"**Your answer:** (fill in here)\".\n",
    "5. Never copy/paste any notebook cells. Inserting new cells is allowed, but it should not be necessary.\n",
    "6. The notebook contains some hidden metadata which is important during our grading process. **Make sure not to corrupt any of this metadata!** The metadata may for example be corrupted if you copy/paste any notebook cells, or if you perform an unsuccessful git merge / git pull. It may also be pruned completely if using Google Colab, so watch out for this. Searching for \"nbgrader\" when opening the notebook in a text editor should take you to the important metadata entries.\n",
    "7. Although we will try our very best to avoid this, it may happen that bugs are found after an assignment is released, and that we will push an updated version of the assignment to GitHub. If this happens, it is important that you update to the new version, while making sure the notebook metadata is properly updated as well. The safest way to make sure nothing gets messed up is to start from scratch on a clean updated version of the notebook, copy/pasting your code from the cells of the previous version into the cells of the new version.\n",
    "8. If you need to have multiple parallel versions of this notebook, make sure not to move them to another directory.\n",
    "9. Although not forced to work exclusively in the course `conda` environment, you need to make sure that the notebook will run in that environment, i.e. that you have not added any additional dependencies.\n",
    "\n",
    "**FOR HA1, HA2 ONLY:** Failing to meet any of these requirements might lead to either a subtraction of points (at best) or a request for resubmission (at worst).\n",
    "\n",
    "We advise you to perform the following steps before submission to ensure that requirements 1, 2, and 3 are always met: **Restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). This might require a bit of time, so plan ahead for this. Finally press the \"Save and Checkout\" button before handing in, to make sure that all your changes are saved to this .ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6bb874a16c1ff767ac0f37ce0491265",
     "grade": false,
     "grade_id": "cell-774c93bf6433de68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in name of notebook file\n",
    "This might seem silly, but the version check below needs to know the filename of the current notebook, which is not trivial to find out programmatically.\n",
    "\n",
    "You might want to have several parallel versions of the notebook, and it is fine to rename the notebook as long as it stays in the same directory. **However**, if you do rename it, you also need to update its own filename below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_fname = \"XXX.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "270e43e75da54d7fb8afbda64083f4e3",
     "grade": false,
     "grade_id": "cell-5676bcf768a7f9be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in group number and member names (use NAME2 and GROUP only for HA1 and HA2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME1 = \"\"\n",
    "NAME2 = \"\"\n",
    "GROUP = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42f960a95815e1aa3ce8132fcec59cd9",
     "grade": false,
     "grade_id": "cell-a15fe781533d9590",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72b2403e87a33f87371b150984248355",
     "grade": false,
     "grade_id": "cell-2b9c2390ee464c39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from platform import python_version_tuple\n",
    "\n",
    "assert (\n",
    "    python_version_tuple()[:2] == (\"3\", \"11\")\n",
    "), \"You are not running Python 3.11. Make sure to run Python through the course Conda environment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15ec4309f1e85f6e17bda73b9b6f48a2",
     "grade": false,
     "grade_id": "cell-4869b45600ce82f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check that notebook server has access to all required resources, and that notebook has not moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c741699084b67aa21d06ff931465b378",
     "grade": false,
     "grade_id": "cell-122ac3d9100b8afb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "nb_dirname = os.path.abspath(\"\")\n",
    "assignment_name = os.path.basename(nb_dirname)\n",
    "assert assignment_name in [\n",
    "    \"IHA1\",\n",
    "    \"IHA2\",\n",
    "    \"HA1\",\n",
    "    \"HA2\",\n",
    "], \"[ERROR] The notebook appears to have been moved from its original directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f09f40b5350db83232189137c550f0a1",
     "grade": false,
     "grade_id": "cell-2455deee513cd39c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify correct nb_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1709bd6d2b55a83969e44d70763b1167",
     "grade": false,
     "grade_id": "cell-0472e2fd710f1d72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "try:\n",
    "    display(\n",
    "        HTML(\n",
    "            r'<script>if(\"{nb_fname}\" != IPython.notebook.notebook_name) {{ alert(\"You have filled in nb_fname = \\\"{nb_fname}\\\", but this does not seem to match the notebook filename \\\"\" + IPython.notebook.notebook_name + \"\\\".\"); }}</script>'.format(\n",
    "                nb_fname=nb_fname\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "except NameError:\n",
    "    assert False, \"Make sure to fill in the nb_fname variable above!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98d88d8e8da19693053764f29dcc591d",
     "grade": false,
     "grade_id": "cell-ceacb1adcae4783d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify that your notebook is up-to-date and not corrupted in any way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f405c9cd7b9720915f79dba54c89375",
     "grade": false,
     "grade_id": "cell-f5a59288e11b4aec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from ha_utils import check_notebook_uptodate_and_not_corrupted\n",
    "\n",
    "check_notebook_uptodate_and_not_corrupted(nb_dirname, nb_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the project structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper notebook to create the folder structure necessary for HA1.\n",
    "Start looking at `HA1.ipynb` and revisit this notebook when needed.\n",
    "\n",
    "This should be run from the same folder where the `dogs-vs-cats.zip` file you downloaded from Kaggle is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dealing with files we use the built-in python module `Path`\n",
    "# It provides a nice abstraction of the file system, compared to working with strings only.\n",
    "# It also makes your code more portable, i.e. easier to share with someone using another operating system.\n",
    "# Some file system operation are not covered by 'Path' and we use 'shutil' for that\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# For splitting the data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data unpacking and re-organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: This script assumes that you have the `dogs-vs-cats.zip` in the same directory as this notebook, but you can set the path to the data directory manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd()\n",
    "zip_file = data_path / \"dogs-vs-cats.zip\"\n",
    "\n",
    "if not ((data_path / \"test\").exists() and (data_path / \"train_all\").exists()):\n",
    "    if not zip_file.exists():\n",
    "        raise FileNotFoundError(\n",
    "            \"Download and place `{}` in the current directory (`{}`)\".format(\n",
    "                zip_file.name, data_path\n",
    "            )\n",
    "        )\n",
    "    # This is a list of all the directories and files this notebook will produce.\n",
    "    # If you have run this before, we will delete them and start over from `dogs-vs-cats.zip`\n",
    "    # Notice how we use the `map` function to conveniently run `Path(<filename>)` on all strings in our list,\n",
    "    # to turn them in portable filepaths.\n",
    "    pre_existing_items = map(\n",
    "        lambda x: data_path / Path(x),\n",
    "        [\n",
    "            \"test1.zip\",\n",
    "            \"test\",\n",
    "            \"val\",\n",
    "            \"train.zip\",\n",
    "            \"train\",\n",
    "            \"train_all\",\n",
    "            \"sampleSubmission.csv\",\n",
    "            \"small_train\",\n",
    "            \"small_val\",\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    for item in pre_existing_items:\n",
    "        if item.exists():\n",
    "            # We need to use different functions for files and directories.\n",
    "            if item.is_dir():\n",
    "                shutil.rmtree(item)\n",
    "            elif item.is_file():\n",
    "                item.unlink()\n",
    "            else:\n",
    "                print(\"Unknown item: {}, remove manually\".format(item))\n",
    "    \n",
    "    \n",
    "    # Depending on your machine the following might take some seconds to run\n",
    "    shutil.unpack_archive(data_path / Path(\"dogs-vs-cats.zip\"), data_path)\n",
    "    shutil.unpack_archive(data_path / Path(\"test1.zip\"), data_path)\n",
    "    shutil.unpack_archive(data_path / Path(\"train.zip\"), data_path)\n",
    "    \n",
    "    (data_path / Path(\"test1\")).rename(data_path / \"test\")\n",
    "    (data_path / Path(\"train\")).rename(data_path / \"train_all\")\n",
    "    \n",
    "    \n",
    "    # Remove sub zip filess\n",
    "    (data_path / Path(\"test1.zip\")).unlink()\n",
    "    (data_path / Path(\"train.zip\")).unlink()\n",
    "    \n",
    "    # Take a look at your current directory. Apart from notebook files (those ending in *.ipynb) you should see\n",
    "    # dogs-vs-cats.zip\n",
    "    # sampleSubmission.csv\n",
    "    # test\n",
    "    # train_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll examine the data inside the directory `train_all`.\n",
    "It contains files like this:\n",
    "\n",
    "```\n",
    "<id>.jpg\n",
    "<id>.jpg\n",
    "```\n",
    "where each id is on the from `<label>.<number>`, e.g. `cat.123`.\n",
    "\n",
    "Inside the directory `test` are unlabelled images\n",
    "\n",
    "```\n",
    "<id>.jpg\n",
    "```\n",
    "where each id is a single number.\n",
    "\n",
    "Predictions on these unknown images is what you would submit to participate in the contest.\n",
    "\n",
    "Let's count them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_path = data_path / \"train_all\"\n",
    "\n",
    "# Get a list of all filenames inside (these will be used for training and validation)\n",
    "# The asterisk '*' is a so called wildcard, i.e. we tell the `glob` method to find any cat/dog images,\n",
    "# regardless of their id.\n",
    "all_cat_filenames = list(train_all_path.glob(\"cat.*.jpg\"))\n",
    "all_dog_filenames = list(train_all_path.glob(\"dog.*.jpg\"))\n",
    "\n",
    "test_path = data_path / \"test\"\n",
    "all_test_filenames = list(test_path.glob(\"*.jpg\"))\n",
    "print(f\"Found {len(all_cat_filenames)} images of cats.\")\n",
    "print(f\"Found {len(all_dog_filenames)} images of dogs.\")\n",
    "print(f\"Found {len(all_test_filenames)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a smaller dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create `'small_train'` and `'small_val'` folders for a smaller subset of the original dataset (the assignment asks for 10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a subset of the entire training dataset (10%)\n",
    "_, few_cat_filenames, _, few_dog_filenames = train_test_split(all_cat_filenames, all_dog_filenames, test_size=0.1, random_state=1)\n",
    "print(f\"The smaller dataset has {len(few_cat_filenames)} images of cats and {len(few_dog_filenames)} images of dogs .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split it into training and validation sets\n",
    "split_ratio_small_dataset = 0.3\n",
    "\n",
    "(few_cat_filenames_train, few_cat_filenames_val,\n",
    "     few_dog_filenames_train, few_dog_filenames_val) = train_test_split(few_cat_filenames,\n",
    "                                                                       few_dog_filenames,\n",
    "                                                                       test_size=split_ratio_small_dataset,\n",
    "                                                                       random_state=2)\n",
    "\n",
    "print(f\"The smaller dataset will be comprised of:\")\n",
    "print(f\"Training:\\t{len(few_cat_filenames_train)} cats and {len(few_dog_filenames_train)} dogs\")\n",
    "print(f\"Validation:\\t{len(few_cat_filenames_val)} cats and {len(few_dog_filenames_val)} dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train and val directories and subdirectories\n",
    "subdirectories = {\n",
    "    data_path / \"small_train/cats\": few_cat_filenames_train,\n",
    "    data_path / \"small_train/dogs\": few_dog_filenames_train,\n",
    "    data_path / \"small_val/cats\": few_cat_filenames_val,\n",
    "    data_path / \"small_val/dogs\": few_dog_filenames_val,\n",
    "}\n",
    "\n",
    "for subdirectory in subdirectories.keys():\n",
    "    subdirectory = Path(subdirectory)\n",
    "    subdirectory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Put the training and validation data in the respective folders\n",
    "def fill_sub_dir(sub_dir, file_subset):\n",
    "    \"\"\"This function copies files from the `train_all` to a `<sub_dir>`\n",
    "    A more efficient solution would be to use \"symbolic links\" (see https://kb.iu.edu/d/abbe)\n",
    "    but for simplicity hard copies is used instead.\n",
    "    \"\"\"\n",
    "    for file in file_subset:\n",
    "        file_path = data_path / sub_dir / file.name\n",
    "        shutil.copyfile(file, file_path)\n",
    "\n",
    "\n",
    "for sub_dir, file_subset in subdirectories.items():\n",
    "    fill_sub_dir(sub_dir, file_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a bigger dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the `val` and `train` folders for the bigger dataset. Note that this dataset will still be a portion of the entire dataset (50%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a subset of the entire training dataset (50%)\n",
    "_, big_cat_filenames, _, big_dog_filenames = train_test_split(\n",
    "    all_cat_filenames, all_dog_filenames, test_size=0.5, random_state=1\n",
    ")\n",
    "print(f\"The bigger dataset has {len(big_cat_filenames)} images of cats and {len(big_dog_filenames)} images of dogs .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the train/val split (to something reasonable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio_big_dataset = None  # Fill in here\n",
    "\n",
    "if split_ratio_big_dataset is None:\n",
    "    raise ValueError(\"`split_ratio_big_dataset` must have a value between 0 and 1.\")\n",
    "\n",
    "# Split it\n",
    "(big_cat_filenames_train, big_cat_filenames_val,\n",
    "     big_dog_filenames_train, big_dog_filenames_val) = train_test_split(big_cat_filenames,\n",
    "                                                                       big_dog_filenames,\n",
    "                                                                       test_size=split_ratio_big_dataset,\n",
    "                                                                       random_state=3)\n",
    "\n",
    "print(\"The bigger dataset will be comprised of:\")\n",
    "print(f\"Train:\\t{len(big_cat_filenames_train)} cats and {len(big_dog_filenames_train)} dogs.\")\n",
    "print(f\"Val:\\t{len(big_cat_filenames_val)} cats and {len(big_dog_filenames_val)} dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train and val directories and subdirectories\n",
    "subdirectories = {\n",
    "    data_path / \"train/cats\": big_cat_filenames_train,\n",
    "    data_path / \"train/dogs\": big_dog_filenames_train,\n",
    "    data_path / \"val/cats\": big_cat_filenames_val,\n",
    "    data_path / \"val/dogs\": big_dog_filenames_val,\n",
    "}\n",
    "\n",
    "for subdirectory in subdirectories.keys():\n",
    "    subdirectory = Path(subdirectory)\n",
    "    subdirectory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for sub_dir, file_subset in subdirectories.items():\n",
    "    fill_sub_dir(sub_dir, file_subset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
