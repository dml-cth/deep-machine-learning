{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b52e372ea25526bbc56123d0ee832337",
     "grade": false,
     "grade_id": "cell-d486a2d083662f95",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# CL2 - Intro to Convolution Neural Networks in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in CL1, Pytorch will be essential in this course.\n",
    "In this computer lab we'll dig deeper into this module with an example of how to define, train, and assess the performance of convolutional neural networks, using Pytorch. For this task, we will use the MNIST dataset, which is comprised of 70.000 28x28 grayscale images of handwritten digits (60k for training, 10k for testing).\n",
    "\n",
    "Our goal is to build a convolutional neural network that takes as input the grayscale image of a handwritten digit, and outputs its corresponding label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Loading and visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by downloading the MNIST dataset using the `MNIST` function. If this is the first time you're running this cell, you will need internet access to download the dataset.\n",
    "\n",
    "MNIST is a popular dataset, that is often used for prototyping and demonstrations. That is why Pytorch comes with a predefined MNIST dataset. In general, it is a lot more work to create a dataset.\n",
    "\n",
    "Recently, there has been some issues with the official hosting of `MNIST`, and you can actually get a small taste of the tedious work of dataset management.\n",
    "If the below command does not work out of the box, you will need to manually do a few steps:\n",
    "- Download the `MNIST`, e.g., from [here](https://deepai.org/dataset/mnist)\n",
    "- Create a folder `MNIST/raw/` in the current folder, i.e., the `CL2` folder where this notebook is in.\n",
    "- Move the downloaded zip file to the `MNIST/raw/` folder and unzip it. Your file structure should now look like this:\n",
    "  ```bash\n",
    "  MNIST\n",
    "  └── raw\n",
    "    ├── t10k-images-idx3-ubyte.gz\n",
    "    ├── t10k-labels-idx1-ubyte.gz\n",
    "    ├── train-images-idx3-ubyte.gz\n",
    "    └── train-labels-idx1-ubyte.gz\n",
    "  ```\n",
    "- Rerun the below command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_data = datasets.MNIST('./', train=True, download=True, transform=transforms.ToTensor())\n",
    "x_train = train_data.data\n",
    "y_train = train_data.targets\n",
    "\n",
    "test_data = datasets.MNIST('./', train=False, download=True, transform=transforms.ToTensor())\n",
    "x_test = test_data.data\n",
    "y_test = test_data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the data.\n",
    "Pytorch is built on top of the more basic module Numpy.\n",
    "Because of this, the `torch.Tensor` object has many of the methods you recognise from a `numpy array`,\n",
    "such as `shape`, `sum()`, `min()`, `max()` et c.\n",
    "\n",
    "We can see the range of pixel values by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Min: {}\\nMax: {}'.format(x_train.min(), x_train.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the shapes of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of `x_train` can be interpreted as (samples, width, height). So we can see that the training set is comprised indeed of 60k images, each 28x28. Doing the same for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shows us that we have 10k images in the test set, also 28x28. We can take a look at one of the examples in the training set by simply indexing `x_train` in its first dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which shows the 11th (remember, arrays are zero-indexed) data point in the training set as a matrix of numbers (note that the numbers range from 0 to 255, we'll normalize that later for easier training). Since we know that each example is actually a grayscale image of a handwritten digit, is more conveninent to display it as an image instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[10], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the corresponding ground truth label by indexing `y_train` with the same index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to normalize the inputs to range from 0 to 1 (instead of 0 to 255):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm = x_train.float() / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Be careful not to run the above cell more than once)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to get the correct dimensions in the inputs. Pytorch's [Conv2D](https://pytorch.org/docs/stable/nn.html#conv2d) layer expects inputs with dimension `(N, C, H, W)`, where `N` is the number of samples, `C` is the number of channels in the image (e.g. 3 for RGB images) whereas `H` and `W` are the height and width, respectively.\n",
    "\n",
    "The current size of the inputs is `(60k, 28, 28)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, H, W = x_train_norm.shape\n",
    "print(\"Dataset shape: {}\".format(x_train_norm.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore need to add a new dimension corresponding to the number of channels.\n",
    "The MNIST data is greyscale, i.e. there is only a single channel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1\n",
    "x_train_norm = x_train_norm.reshape((N, C, H, W))\n",
    "print(\"Dataset shape with explicit channel dim.: {}\".format(x_train_norm.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Using a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this computer lab, having a GPU will accelerate the training considerably. You can check if Pytorch can use a GPU in your computer by using the following method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which returns `True` if Pytorch can use a GPU, `False` otherwise. Having a GPU is not mandatory for this computer lab, so if you don't have one don't worry. In order for the following code to work in computers with and without GPUs, we create a variable that decides automatically which device is being used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Datasets, data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in CL1, we will create data loaders for simplifying our training code.\n",
    "This time, it actually makes sense to do a dataloader.\n",
    "Reading all 60k images into memory when we only process them in batches would be very wasteful.\n",
    "Granted, these images are very low-resolution and in grayscale but we could just as easily have had a dataset with high-resolution colour images. Then our computer would run out of RAM and crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch objects have a `.to(<device>)` method that moves the object to the device we have chosen (cpu or gpu).\n",
    "# All objects involved in the training, like data and models, must be moved to the same device.\n",
    "dataset = TensorDataset(x_train_norm.to(device), y_train.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(dataset)\n",
    "\n",
    "# We split the training data into 70% for training, 30% for validation.\n",
    "n_train_samples = int(n_samples*0.7)\n",
    "n_val_samples = n_samples - n_train_samples\n",
    "# We again use the random_split function to get to random subsets of the data.\n",
    "train_dataset, val_dataset = random_split(dataset, [n_train_samples, n_val_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=n_val_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are not shuffling the data in the validation set (no need to do so), and we are setting the batch size of it to the total number of validation samples (this number should be as big as your memory allows, for faster validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we will use a simple CNN architecture: two convolutional layers (followed by ReLUs and max-pooling), and two fully-connected layers.\n",
    "\n",
    "Note the similarity to the models we created in CL1:\n",
    "\n",
    "- Our model inherits from `nn.Module`\n",
    "- The `__init__` method defines the layers and structure of the network\n",
    "- We provide a `forward` method which defines how the input `x` is mapped to the output `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note 1: the input dimensions of the first fully-connected layer (in this case `4*4*50`) depend both on the size of the input (`28x28`), and on the convolutions and max-poolings we do with it. \n",
    "\n",
    "Note 2: In contrast to the logistic regression task in CL1, the output of the last fully-connected layer is being passed through a (log) softmax layer. Because of this choice, the loss we will use will be Pytorch's negative log-likelihood loss, [`NLLLoss`](https://pytorch.org/docs/stable/nn.html#nllloss). Both alternatives are equivalent (`log_softmax` output + `NLLLoss`, or just `CrossEntropyLoss`), and we show both for completeness).\n",
    "\n",
    "**If you do not remember the difference between `NLLLoss` and `CrossEntropyLoss`, return to the explanation in CL1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Compute the dimensions of the input as it passes through each layer of the network. How should the dimensions of the first fully connected layer change if we change the kernel_size of the convolutions to 7? What if we change the kernel of the first ReLU to `3x3`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the model (and move its parameters to the GPU if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DigitClassifierNetwork()\n",
    "# We moved the dataset to `device`, then we must move model as well.\n",
    "# Don't worry, if you forget it pytorch will throw an error and your code will not run before you fix it.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Setting up the optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained before, we will use the negative log-likelihood loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we also use the popular Adam optimizer with the learning rate 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "# We provide the `Adam` optimizer with the model parameters,\n",
    "# effectively telling it which parameters we consider trainable.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model just like we did in CL1. For conciseness in the training loop, we'll define a function that computes the loss and the accuracy of a given model in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(val_data_loader, model, loss_fn):\n",
    "    losses = []\n",
    "    n_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for b_x, b_y in val_data_loader:\n",
    "            pred = model(b_x)\n",
    "            loss = loss_fn(pred, b_y)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            hard_preds = pred.argmax(dim=1)\n",
    "            n_correct += torch.sum(pred.argmax(dim=1) == b_y).item()\n",
    "        val_accuracy = n_correct/len(val_dataset)\n",
    "        val_avg_loss = sum(losses)/len(losses)    \n",
    "    \n",
    "    return val_accuracy, val_avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this helper function, the training loop becomes (note that if you're not using a GPU, the following cell will take a while to run and it may be better to directly skip to the code presented in the next section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    losses = []\n",
    "    n_correct = 0\n",
    "    for b_x, b_y in train_data_loader:\n",
    "        \n",
    "        # Compute predictions and losses\n",
    "        pred = model(b_x)\n",
    "        loss = loss_fn(pred, b_y)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Count number of correct predictions\n",
    "        hard_preds = pred.argmax(dim=1)\n",
    "        n_correct += torch.sum(pred.argmax(dim=1) == b_y).item()\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()    \n",
    "        \n",
    "    # Compute accuracy and loss in the entire training set\n",
    "    train_accuracy = n_correct/len(train_dataset)\n",
    "    train_avg_loss = sum(losses)/len(losses)    \n",
    "        \n",
    "    # Compute accuracy and loss in the entire validation set\n",
    "    val_accuracy, val_avg_loss = evaluate_model(val_data_loader, model, loss_fn)\n",
    "        \n",
    "    # Display metrics\n",
    "    display_str = 'Epoch {} '\n",
    "    display_str += '\\tLoss: {:.3f} '\n",
    "    display_str += '\\tLoss (val): {:.3f}'\n",
    "    display_str += '\\tAccuracy: {:.2f} '\n",
    "    display_str += '\\tAccuracy (val): {:.2f}'\n",
    "    print(display_str.format(epoch, train_avg_loss, val_avg_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the accuracy and loss in the training set are actually the averages across all batches for each epoch. Since the model is changing during these batches, these metrics don't correspond to any single model during the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Training the model (with frequent progress displays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not using a GPU, you might prefer to display the metrics more often than once per epoch, in order to obtain more frequent feedback about the training. Changing the training loop to achieve this is reasonably straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the model and the optimizer\n",
    "model = DigitClassifierNetwork()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(5):\n",
    "    print('------ Epoch {} ------'.format(epoch))\n",
    "    for i, (b_x, b_y) in enumerate(train_data_loader):\n",
    "        \n",
    "        # Compute predictions and losses\n",
    "        pred = model(b_x)\n",
    "        loss = loss_fn(pred, b_y)\n",
    "        \n",
    "        # Count number of correct predictions\n",
    "        hard_preds = pred.argmax(dim=1)\n",
    "        n_correct = torch.sum(pred.argmax(dim=1) == b_y).item()\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Every 10 batches, display progress\n",
    "        if i % 10 == 0:\n",
    "            display_str = 'Batch {} '\n",
    "            display_str += '\\tLoss: {:.3f} '\n",
    "            display_str += '\\tLoss (val): {:.3f}'\n",
    "            display_str += '\\tAccuracy: {:.2f} '\n",
    "            display_str += '\\tAccuracy (val): {:.2f}'\n",
    "            \n",
    "            val_accuracy, val_avg_loss = evaluate_model(val_data_loader, model, loss_fn)\n",
    "            print(display_str.format(i, loss, val_avg_loss, n_correct/len(b_x), val_accuracy))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: What is the main difference between the losses/accuracies displayed like this and in the previous section?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Training the model (with real-time plotting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to use matplotlib to produce real-time plots of the optimization. To do so, we need to use the following [built-in magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html) of Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows you how to update a plot in real-time during the optimization. The essence of it is that after a certain number of batches, we clear the axis using `clear()`, and then plot the new data. This only works if we first turn the interactive mode on, by calling the [`ion()`](https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.ion.html) function from `pyplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure for plotting\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(8,4))\n",
    "plt.ion()\n",
    "plot_interval = 1\n",
    "\n",
    "# Reset the model and the optimizer\n",
    "model = DigitClassifierNetwork()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Create arrays to save all of the metrics throughout training\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(2):\n",
    "    for i, (b_x, b_y) in enumerate(train_data_loader):\n",
    "        # Compute predictions and loss\n",
    "        pred = model(b_x)\n",
    "        loss = loss_fn(pred, b_y)\n",
    "    \n",
    "        # Back-propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute metrics\n",
    "        hard_preds = pred.argmax(dim=1)\n",
    "        n_correct = torch.sum(pred.argmax(dim=1) == b_y).item()\n",
    "        val_accuracy, val_avg_loss = evaluate_model(val_data_loader, model, loss_fn)\n",
    "        \n",
    "        # Save them in the arrays\n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(val_avg_loss)\n",
    "        train_accs.append(n_correct/len(b_x))\n",
    "        val_accs.append(val_accuracy)\n",
    "        \n",
    "        if i % plot_interval == 0:\n",
    "            # Update plots\n",
    "            ax[0].clear()\n",
    "            ax[0].plot(train_losses)\n",
    "            ax[0].plot(val_losses)\n",
    "\n",
    "            ax[1].clear()\n",
    "            ax[1].plot(train_accs)\n",
    "            ax[1].plot(val_accs)\n",
    "\n",
    "            # Add legends and labels\n",
    "            ax[0].set_title('Loss')\n",
    "            ax[0].set_xlabel('Number of batches')\n",
    "            ax[0].legend(['Train', 'Validation'])\n",
    "\n",
    "            ax[1].set_title('Accuracy')\n",
    "            ax[1].set_xlabel('Number of batches')\n",
    "            ax[1].legend(['Train', 'Validation'])\n",
    "            ax[1].set_ylim([0,1])\n",
    "\n",
    "            # Draw the figure on the screen\n",
    "            fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that plotting the training after every single batch (i.e. `plot_interval=1`) adds a considerable overhead to the training process. Instead it's better to only plot after, say, every 10 batches so as to not slow down the optimization too much.\n",
    "\n",
    "Note: there are more efficient ways of accomplishing this type of real-time plotting (for example, using [`set_xdata`](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_xdata) and [`set_ydata`](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_xdata) instead of clearing the plot at every update), but they require more coding and a further understanding of `matplotlib`. We present only this one for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Change the architecture, optimizer, and/or hyper-parameters used during training and see how that affects the final performance of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have found the best model, we can evaluate its performance on the test set. Just like in CL1, we will compute both the accuracy on the test set, and the confusion matrix.\n",
    "\n",
    "In order for our model to work on the data from the test set, we need to pre-process it in exactly the same way we did in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "x_test = x_test.float() / 255\n",
    "\n",
    "# Add dimension\n",
    "x_test = x_test[:, None]\n",
    "\n",
    "# Optionally move to GPU\n",
    "x_test = x_test.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now performing forward propagation in the entire test set is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = model(x_test)\n",
    "print(preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we obtain hard choices with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices_test = preds_test.argmax(dim=1)\n",
    "print(choices_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these with the ground-truth labels gives us the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test = torch.sum(choices_test==y_test).item()/len(y_test)\n",
    "print('Accuracy on the test set is {:.2f}.'.format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we compute the confusion matrix using `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` excpects input as numpy arrays instead of tensors (and also optionally move them back to CPU).\n",
    "Conveniently, Pytorch provides method to convert a tensor to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.cpu().numpy()\n",
    "choices_test = choices_test.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, choices_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: What can you conclude from these? Which classes are the easiest to misclassify? Was that expected?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
